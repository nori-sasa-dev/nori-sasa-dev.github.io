{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c98d86d-5651-4cd4-b812-6aa16a8f63cb",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6f8a31-1eb7-4eea-9991-f274fa02fe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10, 1, 28, 28) # 高さ28、横幅28、1チャネルのデータが10個ある場合の4次元データをランダムに生成\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02e7cb3-bba3-461b-b8a3-3452ce31f9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9643f5a6-621d-4bd0-82eb-bcf17ccfbdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acadac9f-134e-4738-9f39-d49237ca911b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179acc9f-2e8a-4d45-813e-0926fa0763b3",
   "metadata": {},
   "source": [
    "## im2colによる展開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91da3cfc-bea1-41f1-b872-38cda6356918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44b76c1-f2f0-4164-a076-f12599aa9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W     # フィルタ（重み）: shape=(FN, C, FH, FW)\n",
    "        self.b = b     # バイアス: shape=(FN,)\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # 元の配列の軸番号を指定して並べ替える\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84eebc3b-189e-40cb-937a-9ba34cecb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 展開\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        # 最大値\n",
    "        out = np.max(col, axis=1)\n",
    "        # 整形\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) # 元の配列の軸番号を指定して並べ替える\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c06bbf-a2e4-452b-b777-ca3aad89a70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7048e6c4-7e45-4c48-af53-3604e780afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size +2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Poolling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353fdc92-7318-4ecb-8af0-d1a9ca78f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2997335389326103\n",
      "=== epoch:1, train acc:0.317, test acc:0.269 ===\n",
      "train loss:2.2980045461302656\n",
      "train loss:2.292220937845927\n",
      "train loss:2.286274563214618\n",
      "train loss:2.277302168466955\n",
      "train loss:2.2661000383032355\n",
      "train loss:2.244990986036056\n",
      "train loss:2.231061560166686\n",
      "train loss:2.215924718416742\n",
      "train loss:2.1573056075081505\n",
      "train loss:2.1809636581699254\n",
      "train loss:2.1181625030710625\n",
      "train loss:2.0287172180403807\n",
      "train loss:2.0077907259016743\n",
      "train loss:1.968006945492619\n",
      "train loss:1.8781374881857542\n",
      "train loss:1.8382423611183953\n",
      "train loss:1.7167112873907602\n",
      "train loss:1.594627667540906\n",
      "train loss:1.7471547292138103\n",
      "train loss:1.5287080008010199\n",
      "train loss:1.430777026941694\n",
      "train loss:1.3460703633573703\n",
      "train loss:1.232543505648509\n",
      "train loss:1.1830180167531632\n",
      "train loss:1.209068742044047\n",
      "train loss:1.0597311555037525\n",
      "train loss:1.1491217935900955\n",
      "train loss:0.9762075380948316\n",
      "train loss:0.9609342980196983\n",
      "train loss:0.8677564894401034\n",
      "train loss:1.0053257119437127\n",
      "train loss:1.0067874325284354\n",
      "train loss:0.7980283308341063\n",
      "train loss:0.7014603623267835\n",
      "train loss:0.8082382544546967\n",
      "train loss:1.0162412225963933\n",
      "train loss:0.8753602409128369\n",
      "train loss:0.7374117397296522\n",
      "train loss:0.7616354685729945\n",
      "train loss:0.5938528862083305\n",
      "train loss:0.659396436572305\n",
      "train loss:0.6598514779113865\n",
      "train loss:0.7290797264235596\n",
      "train loss:0.7019540416910622\n",
      "train loss:0.46036938074961564\n",
      "train loss:0.5684132503506845\n",
      "train loss:0.7244361945557537\n",
      "train loss:0.5655847836301922\n",
      "train loss:0.5855133419395535\n",
      "train loss:0.4890119336812703\n",
      "train loss:0.5359512388870571\n",
      "train loss:0.6675637646821645\n",
      "train loss:0.4929834058379203\n",
      "train loss:0.5193904732497667\n",
      "train loss:0.7760879536793213\n",
      "train loss:0.6304340021370741\n",
      "train loss:0.47068107697303063\n",
      "train loss:0.5611393877885689\n",
      "train loss:0.6883650586228718\n",
      "train loss:0.5318120418858203\n",
      "train loss:0.4118846666840007\n",
      "train loss:0.664059657738926\n",
      "train loss:0.43614291495369634\n",
      "train loss:0.5491036274772947\n",
      "train loss:0.4310382966015834\n",
      "train loss:0.4042759243842305\n",
      "train loss:0.477573486724528\n",
      "train loss:0.4606375657061975\n",
      "train loss:0.5247213727925939\n",
      "train loss:0.5361850256738147\n",
      "train loss:0.4416776223143678\n",
      "train loss:0.3742170203871194\n",
      "train loss:0.45036098018307924\n",
      "train loss:0.38198809652887133\n",
      "train loss:0.5210305715529813\n",
      "train loss:0.31152234048840505\n",
      "train loss:0.35869115018065967\n",
      "train loss:0.41914899589346655\n",
      "train loss:0.41050518758367166\n",
      "train loss:0.4420067458374014\n",
      "train loss:0.49140889976335267\n",
      "train loss:0.528208806394004\n",
      "train loss:0.43586378509153784\n",
      "train loss:0.46560512109274654\n",
      "train loss:0.37285475347551356\n",
      "train loss:0.5663620578617433\n",
      "train loss:0.47352564250901297\n",
      "train loss:0.4504905640381486\n",
      "train loss:0.4023579256155839\n",
      "train loss:0.41056748280339833\n",
      "train loss:0.47345769514129765\n",
      "train loss:0.30189549917450953\n",
      "train loss:0.48990151871521903\n",
      "train loss:0.30816960178697395\n",
      "train loss:0.5481644468905893\n",
      "train loss:0.3451678664084237\n",
      "train loss:0.31080061676377696\n",
      "train loss:0.3010777041719646\n",
      "train loss:0.26762621508377227\n",
      "train loss:0.22991723800594566\n",
      "train loss:0.5614424584285993\n",
      "train loss:0.35347254392379535\n",
      "train loss:0.3784724097002764\n",
      "train loss:0.40136589111342025\n",
      "train loss:0.6374785895163421\n",
      "train loss:0.2862790897903345\n",
      "train loss:0.5651948189116526\n",
      "train loss:0.5292765683235544\n",
      "train loss:0.4176019022450702\n",
      "train loss:0.4528530411032762\n",
      "train loss:0.20348490529239874\n",
      "train loss:0.5390418391277564\n",
      "train loss:0.3260012929508433\n",
      "train loss:0.4521259618563312\n",
      "train loss:0.41649429659461995\n",
      "train loss:0.3195781958445242\n",
      "train loss:0.46528036880962775\n",
      "train loss:0.3443240021887678\n",
      "train loss:0.24420843764140696\n",
      "train loss:0.2920530884978798\n",
      "train loss:0.30449181329432673\n",
      "train loss:0.3106428216447041\n",
      "train loss:0.34546261384363947\n",
      "train loss:0.27809614937800214\n",
      "train loss:0.30170215223813446\n",
      "train loss:0.3828285063667314\n",
      "train loss:0.38538818619167164\n",
      "train loss:0.3406701273465193\n",
      "train loss:0.3808279887180075\n",
      "train loss:0.2843661136103693\n",
      "train loss:0.2511739352226954\n",
      "train loss:0.40037090494901945\n",
      "train loss:0.2447208278863074\n",
      "train loss:0.4108393791837511\n",
      "train loss:0.41107787468842516\n",
      "train loss:0.3754164302257292\n",
      "train loss:0.27899451500063593\n",
      "train loss:0.26399554894714217\n",
      "train loss:0.2792889755176206\n",
      "train loss:0.2653383605157958\n",
      "train loss:0.21138443984335084\n",
      "train loss:0.3064372483297343\n",
      "train loss:0.27208338880662397\n",
      "train loss:0.2683811073894191\n",
      "train loss:0.26029639230006224\n",
      "train loss:0.4256402875739232\n",
      "train loss:0.27648498065406096\n",
      "train loss:0.33106846124856637\n",
      "train loss:0.2941167736619518\n",
      "train loss:0.36635218042125994\n",
      "train loss:0.3062924808472508\n",
      "train loss:0.2881527561937221\n",
      "train loss:0.38831941602913345\n",
      "train loss:0.3222910973276022\n",
      "train loss:0.285040025072258\n",
      "train loss:0.23335709133997667\n",
      "train loss:0.24992177284297828\n",
      "train loss:0.2850232425691363\n",
      "train loss:0.3174379494514538\n",
      "train loss:0.3784801993242573\n",
      "train loss:0.2271387015707726\n",
      "train loss:0.3545076447250335\n",
      "train loss:0.3119839175176949\n",
      "train loss:0.3426810374045705\n",
      "train loss:0.3190698412214355\n",
      "train loss:0.4224463533952506\n",
      "train loss:0.2680762902845876\n",
      "train loss:0.4625474761543816\n",
      "train loss:0.4905744210703812\n",
      "train loss:0.262867722542961\n",
      "train loss:0.36034394745821285\n",
      "train loss:0.19359224501137248\n",
      "train loss:0.4129026903995547\n",
      "train loss:0.30181286335134605\n",
      "train loss:0.4097966277502231\n",
      "train loss:0.3390332845335668\n",
      "train loss:0.40039570487050663\n",
      "train loss:0.2716854156109101\n",
      "train loss:0.2347100151489367\n",
      "train loss:0.2797290601740863\n",
      "train loss:0.3748486498965087\n",
      "train loss:0.2775525874213447\n",
      "train loss:0.37581157654890807\n",
      "train loss:0.15652952232452358\n",
      "train loss:0.36015044656272077\n",
      "train loss:0.325780084797756\n",
      "train loss:0.2939088847174738\n",
      "train loss:0.38177005973207867\n",
      "train loss:0.26416667822434614\n",
      "train loss:0.13573425247857718\n",
      "train loss:0.2920777862992565\n",
      "train loss:0.2566798937910613\n",
      "train loss:0.2926508078047155\n",
      "train loss:0.3767623852220307\n",
      "train loss:0.31446101512940633\n",
      "train loss:0.3494003871621026\n",
      "train loss:0.285125173308786\n",
      "train loss:0.17546684301236126\n",
      "train loss:0.3770101127572365\n",
      "train loss:0.31580442553593496\n",
      "train loss:0.2668830673145398\n",
      "train loss:0.2882427352796947\n",
      "train loss:0.32553364033471177\n",
      "train loss:0.40697479412986126\n",
      "train loss:0.21833626667196387\n",
      "train loss:0.4740482385066192\n",
      "train loss:0.18867336644928415\n",
      "train loss:0.3355723453552668\n",
      "train loss:0.29362789346330787\n",
      "train loss:0.26355504986373895\n",
      "train loss:0.3434864232182221\n",
      "train loss:0.3005241481775277\n",
      "train loss:0.3342686115317173\n",
      "train loss:0.253027336213748\n",
      "train loss:0.3034439730294444\n",
      "train loss:0.42305245696297544\n",
      "train loss:0.3150401477686542\n",
      "train loss:0.4261655420673938\n",
      "train loss:0.37318561750613355\n",
      "train loss:0.2857109017111744\n",
      "train loss:0.25627407892301063\n",
      "train loss:0.4108288112896239\n",
      "train loss:0.4308011942747438\n",
      "train loss:0.15907764398031668\n",
      "train loss:0.17776211351546645\n",
      "train loss:0.3396105672683854\n",
      "train loss:0.3364138733367028\n",
      "train loss:0.34935969609368805\n",
      "train loss:0.2682849374952873\n",
      "train loss:0.2657179789122574\n",
      "train loss:0.19972299765119547\n",
      "train loss:0.2512784865692171\n",
      "train loss:0.2707317798654198\n",
      "train loss:0.20273342486265758\n",
      "train loss:0.27130684868842575\n",
      "train loss:0.27336795793458224\n",
      "train loss:0.2331861965332527\n",
      "train loss:0.2238926757524305\n",
      "train loss:0.31284855765325936\n",
      "train loss:0.31328787925139595\n",
      "train loss:0.21245124442899357\n",
      "train loss:0.2848502000501748\n",
      "train loss:0.2588810526877705\n",
      "train loss:0.34207431153808865\n",
      "train loss:0.17222286754015323\n",
      "train loss:0.26369095136671145\n",
      "train loss:0.30399539091951083\n",
      "train loss:0.22497477129422958\n",
      "train loss:0.35405430084030765\n",
      "train loss:0.3733365211323118\n",
      "train loss:0.26078084802983936\n",
      "train loss:0.23763425837585742\n",
      "train loss:0.19628744312025787\n",
      "train loss:0.5989337528368975\n",
      "train loss:0.2189887161268351\n",
      "train loss:0.3731626929482348\n",
      "train loss:0.2055845406510348\n",
      "train loss:0.21123511081595023\n",
      "train loss:0.27564271203399193\n",
      "train loss:0.3758662787848721\n",
      "train loss:0.15436900501301176\n",
      "train loss:0.2679784458325786\n",
      "train loss:0.25125507778434775\n",
      "train loss:0.35020665958712427\n",
      "train loss:0.17679935530777133\n",
      "train loss:0.19223494976222316\n",
      "train loss:0.2937937657391269\n",
      "train loss:0.4297854044258546\n",
      "train loss:0.3614559057226643\n",
      "train loss:0.23761615632043906\n",
      "train loss:0.3204723884764265\n",
      "train loss:0.29585198361142556\n",
      "train loss:0.3126438043204482\n",
      "train loss:0.28555590200927633\n",
      "train loss:0.16273383798280222\n",
      "train loss:0.27844036383175214\n",
      "train loss:0.33827806315183717\n",
      "train loss:0.2600613412457924\n",
      "train loss:0.21795038097420621\n",
      "train loss:0.27561981177806394\n",
      "train loss:0.432398652299355\n",
      "train loss:0.21178178911285492\n",
      "train loss:0.2542131512521399\n",
      "train loss:0.17581988979374227\n",
      "train loss:0.14911014229510908\n",
      "train loss:0.23899583522894466\n",
      "train loss:0.31545513571037703\n",
      "train loss:0.18735965700366608\n",
      "train loss:0.20396693382341802\n",
      "train loss:0.3097952003165076\n",
      "train loss:0.10673197657999166\n",
      "train loss:0.30199555975323267\n",
      "train loss:0.31578773524549614\n",
      "train loss:0.17506009439210338\n",
      "train loss:0.25755318175712505\n",
      "train loss:0.23276156958797997\n",
      "train loss:0.4653057575054659\n",
      "train loss:0.1488561241448635\n",
      "train loss:0.15337260278669715\n",
      "train loss:0.17935823192742936\n",
      "train loss:0.22207364905933563\n",
      "train loss:0.17515107042504927\n",
      "train loss:0.2042828785945348\n",
      "train loss:0.2513830341774638\n",
      "train loss:0.24462652072479166\n",
      "train loss:0.2530797414031459\n",
      "train loss:0.19486635124870788\n",
      "train loss:0.18074700114604306\n",
      "train loss:0.40607859491743814\n",
      "train loss:0.24624295487622366\n",
      "train loss:0.15372384522925794\n",
      "train loss:0.18402017667774248\n",
      "train loss:0.3115701373182853\n",
      "train loss:0.2724348628511517\n",
      "train loss:0.2560992980874736\n",
      "train loss:0.1700969149960341\n",
      "train loss:0.3470693072030632\n",
      "train loss:0.24296641449642492\n",
      "train loss:0.16211251816462305\n",
      "train loss:0.23230395726601163\n",
      "train loss:0.1015316744973421\n",
      "train loss:0.30729493160752835\n",
      "train loss:0.23921562795193105\n",
      "train loss:0.16347095875140227\n",
      "train loss:0.17807769182785194\n",
      "train loss:0.21026682697752214\n",
      "train loss:0.17397339729120478\n",
      "train loss:0.21200385102719174\n",
      "train loss:0.21497571639902927\n",
      "train loss:0.30260480615272123\n",
      "train loss:0.2994199754893952\n",
      "train loss:0.23884818689393633\n",
      "train loss:0.41334718134081827\n",
      "train loss:0.26888450262312724\n",
      "train loss:0.18347019172142498\n",
      "train loss:0.17408067145683057\n",
      "train loss:0.11479540196116167\n",
      "train loss:0.26306416849139097\n",
      "train loss:0.24584213487178744\n",
      "train loss:0.23800220425246452\n",
      "train loss:0.17962137697988723\n",
      "train loss:0.2319231803523191\n",
      "train loss:0.18045356877596716\n",
      "train loss:0.12340944775418385\n",
      "train loss:0.24726185163239248\n",
      "train loss:0.21124992106614882\n",
      "train loss:0.275350061855229\n",
      "train loss:0.11872977384529161\n",
      "train loss:0.17145579059307972\n",
      "train loss:0.17598573959336122\n",
      "train loss:0.21617026943355216\n",
      "train loss:0.20473402232072613\n",
      "train loss:0.11981004459744984\n",
      "train loss:0.16314983734656038\n",
      "train loss:0.09544780007171019\n",
      "train loss:0.2788620221452015\n",
      "train loss:0.19147449711991263\n",
      "train loss:0.1691825141119546\n",
      "train loss:0.30004157266538584\n",
      "train loss:0.15635975673516925\n",
      "train loss:0.3438439308645267\n",
      "train loss:0.18714520104728913\n",
      "train loss:0.2680697170621157\n",
      "train loss:0.16111977977385356\n",
      "train loss:0.17155067603961308\n",
      "train loss:0.1303855716110568\n",
      "train loss:0.22591353655298727\n",
      "train loss:0.1635580426991628\n",
      "train loss:0.23881327692640478\n",
      "train loss:0.2222420533345364\n",
      "train loss:0.1799565201647947\n",
      "train loss:0.10708663553986492\n",
      "train loss:0.24915198463338778\n",
      "train loss:0.17431602542831107\n",
      "train loss:0.2527639883732704\n",
      "train loss:0.2526539825978513\n",
      "train loss:0.13743219924558792\n",
      "train loss:0.1256884200648667\n",
      "train loss:0.1781349501862809\n",
      "train loss:0.24899341544824594\n",
      "train loss:0.22560286147916297\n",
      "train loss:0.21932328663518824\n",
      "train loss:0.07802255137316973\n",
      "train loss:0.2471804639183684\n",
      "train loss:0.17230446341591374\n",
      "train loss:0.14779425944609179\n",
      "train loss:0.17373701801605168\n",
      "train loss:0.17599044509245768\n",
      "train loss:0.23246932807921408\n",
      "train loss:0.1926559937483636\n",
      "train loss:0.1566959146271356\n",
      "train loss:0.11795736185529009\n",
      "train loss:0.1334399676438455\n",
      "train loss:0.0825550553109051\n",
      "train loss:0.20113930446913716\n",
      "train loss:0.09561013665600437\n",
      "train loss:0.15602286035663637\n",
      "train loss:0.28783699335492763\n",
      "train loss:0.09699803297916398\n",
      "train loss:0.1288160163630821\n",
      "train loss:0.2859200795591119\n",
      "train loss:0.136524619602021\n",
      "train loss:0.1519632057658314\n",
      "train loss:0.1893100932960065\n",
      "train loss:0.25034563036925145\n",
      "train loss:0.11887310622519846\n",
      "train loss:0.26374825636436666\n",
      "train loss:0.2371992936075692\n",
      "train loss:0.22005897571747338\n",
      "train loss:0.2815649112876578\n",
      "train loss:0.20944431800540853\n",
      "train loss:0.1979488494445143\n",
      "train loss:0.19276438962981612\n",
      "train loss:0.14127695295645679\n",
      "train loss:0.13338475384294432\n",
      "train loss:0.12191791099410301\n",
      "train loss:0.17585925075507167\n",
      "train loss:0.2541989632194657\n",
      "train loss:0.20944216347283084\n",
      "train loss:0.23629763971127787\n",
      "train loss:0.20954051458933173\n",
      "train loss:0.09847996839351915\n",
      "train loss:0.1846715220173244\n",
      "train loss:0.18978758241736898\n",
      "train loss:0.20964476771348342\n",
      "train loss:0.22143685327885668\n",
      "train loss:0.09101732586191359\n",
      "train loss:0.07641878381013761\n",
      "train loss:0.2047503565848248\n",
      "train loss:0.13533245979927683\n",
      "train loss:0.14600194789667462\n",
      "train loss:0.1974239412308739\n",
      "train loss:0.21131877900034846\n",
      "train loss:0.08820966817791087\n",
      "train loss:0.1538918653718369\n",
      "train loss:0.12751880028148554\n",
      "train loss:0.06790716397370593\n",
      "train loss:0.08622156955229032\n",
      "train loss:0.215820843103026\n",
      "train loss:0.31984319458516414\n",
      "train loss:0.25525985777278637\n",
      "train loss:0.08220789291252746\n",
      "train loss:0.12313011763828774\n",
      "train loss:0.2142837407942853\n",
      "train loss:0.11125618605506395\n",
      "train loss:0.19846128686130649\n",
      "train loss:0.09937687988522168\n",
      "train loss:0.19369908604150357\n",
      "train loss:0.0987975090683264\n",
      "train loss:0.1117985793617044\n",
      "train loss:0.2612631936526792\n",
      "train loss:0.2195984318383461\n",
      "train loss:0.1777942071819722\n",
      "train loss:0.21902282926977676\n",
      "train loss:0.14459972235218632\n",
      "train loss:0.16135138450480324\n",
      "train loss:0.2389400083485995\n",
      "train loss:0.11073149832178694\n",
      "train loss:0.0850154388926844\n",
      "train loss:0.1770070157755433\n",
      "train loss:0.15154960778411872\n",
      "train loss:0.16726734905045407\n",
      "train loss:0.1927404689351002\n",
      "train loss:0.14134242493052862\n",
      "train loss:0.10149332292763973\n",
      "train loss:0.15169429608816742\n",
      "train loss:0.15483327460476407\n",
      "train loss:0.22541929844762365\n",
      "train loss:0.3249322777514114\n",
      "train loss:0.18993406098319995\n",
      "train loss:0.19267969046770844\n",
      "train loss:0.16918106815193212\n",
      "train loss:0.20149235670167584\n",
      "train loss:0.1596904259638213\n",
      "train loss:0.10694267950390417\n",
      "train loss:0.2911545038157971\n",
      "train loss:0.11397545033853529\n",
      "train loss:0.1474681781340088\n",
      "train loss:0.20665201760881874\n",
      "train loss:0.09236469219350188\n",
      "train loss:0.15533129342441435\n",
      "train loss:0.15891383560018565\n",
      "train loss:0.14488573674601718\n",
      "train loss:0.21906591608513332\n",
      "train loss:0.11495551365471339\n",
      "train loss:0.16538432316937232\n",
      "train loss:0.26157592031138394\n",
      "train loss:0.09532688262591947\n",
      "train loss:0.09673541101413898\n",
      "train loss:0.2109346882947289\n",
      "train loss:0.18422919890235628\n",
      "train loss:0.17312671106981092\n",
      "train loss:0.15790306492743963\n",
      "train loss:0.15654843518437647\n",
      "train loss:0.16474420863884046\n",
      "train loss:0.19678781861198835\n",
      "train loss:0.35106545176856974\n",
      "train loss:0.14998627274450155\n",
      "train loss:0.12569671507107066\n",
      "train loss:0.15326881525451816\n",
      "train loss:0.09550501262755748\n",
      "train loss:0.17176943691911986\n",
      "train loss:0.16610926899111209\n",
      "train loss:0.09088772491328108\n",
      "train loss:0.08362742620861505\n",
      "train loss:0.08896232713341223\n",
      "train loss:0.2561839154800137\n",
      "train loss:0.09452832823231021\n",
      "train loss:0.12052001751288173\n",
      "train loss:0.16536511971041962\n",
      "train loss:0.14103779163713764\n",
      "train loss:0.20576952520229752\n",
      "train loss:0.1881224386740423\n",
      "train loss:0.170748319699781\n",
      "train loss:0.194869985588\n",
      "train loss:0.24833940193390053\n",
      "train loss:0.10166567512915485\n",
      "train loss:0.11233863416803848\n",
      "train loss:0.24672748061787517\n",
      "train loss:0.13053468554831135\n",
      "train loss:0.20361217460287048\n",
      "train loss:0.12929194584014303\n",
      "train loss:0.1388139963187084\n",
      "train loss:0.11675679872329178\n",
      "train loss:0.23411882259607433\n",
      "train loss:0.18522544327844914\n",
      "train loss:0.08390687990543658\n",
      "train loss:0.16413408152725664\n",
      "train loss:0.16527575279308515\n",
      "train loss:0.22825054505964798\n",
      "train loss:0.09786568885978827\n",
      "train loss:0.17123009022698799\n",
      "train loss:0.08926861989038025\n",
      "train loss:0.22488101782561715\n",
      "train loss:0.20336787652507304\n",
      "train loss:0.10075615449302443\n",
      "train loss:0.21080177333768244\n",
      "train loss:0.11990199402292027\n",
      "train loss:0.2383337053520407\n",
      "train loss:0.24625206163718003\n",
      "train loss:0.12351239852361323\n",
      "train loss:0.16362789534728772\n",
      "train loss:0.22517297036268413\n",
      "train loss:0.1675419560860367\n",
      "train loss:0.10800672245529842\n",
      "train loss:0.17298789865884387\n",
      "train loss:0.13238962862062578\n",
      "train loss:0.1420464369111528\n",
      "train loss:0.07407344764732947\n",
      "train loss:0.1908055196565867\n",
      "train loss:0.12163706666612553\n",
      "train loss:0.117399688192214\n",
      "train loss:0.1307075402490928\n",
      "train loss:0.0701295230984657\n",
      "train loss:0.15053113715487737\n",
      "train loss:0.16515774552487922\n",
      "train loss:0.21019256345372636\n",
      "train loss:0.11994060514838618\n",
      "train loss:0.09064592675501737\n",
      "train loss:0.20472249018705022\n",
      "train loss:0.08358643656736907\n",
      "train loss:0.1841000891851287\n",
      "train loss:0.10725510355761186\n",
      "train loss:0.1104924177884837\n",
      "train loss:0.15049914718695973\n",
      "train loss:0.25672166269580476\n",
      "train loss:0.30707070732412417\n",
      "train loss:0.08744885356410415\n",
      "train loss:0.13553717165665324\n",
      "train loss:0.21100894828588118\n",
      "train loss:0.1555575251304362\n",
      "train loss:0.10192659358006892\n",
      "train loss:0.183140986131087\n",
      "train loss:0.16390536400058994\n",
      "train loss:0.20936837260452154\n",
      "train loss:0.18012536419587197\n",
      "train loss:0.1502781985958695\n",
      "train loss:0.13897009522729925\n",
      "train loss:0.1400550364262649\n",
      "train loss:0.12566635848016341\n",
      "train loss:0.20695412269002414\n",
      "train loss:0.1235865381157232\n",
      "train loss:0.16725638952556682\n",
      "train loss:0.13485126595669153\n",
      "train loss:0.1696816490981275\n",
      "train loss:0.09515817629980289\n",
      "train loss:0.12016059293376712\n",
      "train loss:0.12344671068445406\n",
      "train loss:0.09936758644232001\n",
      "train loss:0.15963395696723823\n",
      "train loss:0.17773663906231352\n",
      "train loss:0.33484776281011686\n",
      "train loss:0.16034044101603695\n",
      "train loss:0.2367506876438266\n",
      "train loss:0.047367044757052626\n",
      "train loss:0.06844129370439857\n",
      "train loss:0.182989727066529\n",
      "train loss:0.11074918619570334\n",
      "train loss:0.061713831051767344\n",
      "train loss:0.12739479541847415\n",
      "=== epoch:2, train acc:0.951, test acc:0.962 ===\n",
      "train loss:0.18491062824450935\n",
      "train loss:0.11880625357460672\n",
      "train loss:0.16562594901683522\n",
      "train loss:0.16844411529255773\n",
      "train loss:0.1369519225688961\n",
      "train loss:0.11988064991511825\n",
      "train loss:0.07556503800058567\n",
      "train loss:0.08192144222639128\n",
      "train loss:0.08774747729172228\n",
      "train loss:0.10961265809899277\n",
      "train loss:0.0955817293789219\n",
      "train loss:0.18758494412813564\n",
      "train loss:0.11611714690120613\n",
      "train loss:0.1697567130845349\n",
      "train loss:0.08542067381115194\n",
      "train loss:0.17530837420234793\n",
      "train loss:0.11028805175806891\n",
      "train loss:0.11575514790876218\n",
      "train loss:0.15148810606419671\n",
      "train loss:0.16960588042328037\n",
      "train loss:0.07426584241810695\n",
      "train loss:0.1658772826359296\n",
      "train loss:0.07946512423041846\n",
      "train loss:0.1330449475433274\n",
      "train loss:0.07308407963286917\n",
      "train loss:0.10000218414627371\n",
      "train loss:0.13139823773733808\n",
      "train loss:0.20296478436169227\n",
      "train loss:0.10246395769308528\n",
      "train loss:0.12258000464204157\n",
      "train loss:0.08945654607402441\n",
      "train loss:0.08560838607257651\n",
      "train loss:0.11930462234964642\n",
      "train loss:0.25458133743348144\n",
      "train loss:0.17383617460998965\n",
      "train loss:0.13748965980499336\n",
      "train loss:0.14998980007428334\n",
      "train loss:0.11936559460200032\n",
      "train loss:0.07353697887215022\n",
      "train loss:0.28123335607717803\n",
      "train loss:0.07773436175612748\n",
      "train loss:0.12437425107819228\n",
      "train loss:0.1711588182009852\n",
      "train loss:0.2813629580256618\n",
      "train loss:0.09807555514182473\n",
      "train loss:0.12210576797550714\n",
      "train loss:0.08460166523229544\n",
      "train loss:0.12053444509943205\n",
      "train loss:0.0829699502790128\n",
      "train loss:0.09518676792994482\n",
      "train loss:0.1334856287161386\n",
      "train loss:0.13783054220435048\n",
      "train loss:0.1267681102523938\n",
      "train loss:0.04976758836162654\n",
      "train loss:0.08149278961349368\n",
      "train loss:0.09037637185627397\n",
      "train loss:0.11554998856899655\n",
      "train loss:0.10696300984281092\n",
      "train loss:0.1443245184159065\n",
      "train loss:0.15524442527324817\n",
      "train loss:0.1601972968054229\n",
      "train loss:0.15657905656389987\n",
      "train loss:0.04196738452293678\n",
      "train loss:0.10771660473044947\n",
      "train loss:0.12126496543787235\n",
      "train loss:0.1571419753144105\n",
      "train loss:0.05843133661618251\n",
      "train loss:0.1642554977770414\n",
      "train loss:0.2314376335867222\n",
      "train loss:0.24118412560549132\n",
      "train loss:0.21212777096853144\n",
      "train loss:0.22472356333385587\n",
      "train loss:0.08978905618311056\n",
      "train loss:0.07608281524267389\n",
      "train loss:0.23898691050959722\n",
      "train loss:0.23927905993045825\n",
      "train loss:0.052137238796475825\n",
      "train loss:0.11888059040534729\n",
      "train loss:0.08611593616382787\n",
      "train loss:0.1372981237915688\n",
      "train loss:0.0855728260402443\n",
      "train loss:0.11823983542244149\n",
      "train loss:0.2725744557209121\n",
      "train loss:0.22803610717650513\n",
      "train loss:0.03680594588026816\n",
      "train loss:0.12856111732861672\n",
      "train loss:0.12905579724751187\n",
      "train loss:0.08753138331989184\n",
      "train loss:0.13211644313496804\n",
      "train loss:0.0986936439520258\n",
      "train loss:0.0975636355973449\n",
      "train loss:0.06289140388737453\n",
      "train loss:0.09544784118478519\n",
      "train loss:0.21957372282034815\n",
      "train loss:0.13147737206813806\n",
      "train loss:0.2414352753124883\n",
      "train loss:0.17455997335082532\n",
      "train loss:0.2617381354180879\n",
      "train loss:0.05627873868596311\n",
      "train loss:0.07044968885882484\n",
      "train loss:0.07813467038522698\n",
      "train loss:0.07016008381550368\n",
      "train loss:0.07553377895220786\n",
      "train loss:0.06635282297308018\n",
      "train loss:0.10532445914097105\n",
      "train loss:0.0619503892818791\n",
      "train loss:0.0707179413672344\n",
      "train loss:0.15227906977085925\n",
      "train loss:0.0898418441959925\n",
      "train loss:0.1111794141464931\n",
      "train loss:0.05297157361490339\n",
      "train loss:0.14321702496855168\n",
      "train loss:0.17861174492398796\n",
      "train loss:0.07656035560727828\n",
      "train loss:0.05490937709640887\n",
      "train loss:0.14102994215428996\n",
      "train loss:0.18880025354659913\n",
      "train loss:0.03659762491540528\n",
      "train loss:0.19154196848893776\n",
      "train loss:0.052238605129186634\n",
      "train loss:0.08323977941570895\n",
      "train loss:0.15773334112252974\n",
      "train loss:0.1262523294071335\n",
      "train loss:0.1244654811387578\n",
      "train loss:0.13417651511572087\n",
      "train loss:0.0755631899978392\n",
      "train loss:0.21419185897502943\n",
      "train loss:0.07370400762312264\n",
      "train loss:0.07568042420583745\n",
      "train loss:0.10952301368429071\n",
      "train loss:0.12548072134300195\n",
      "train loss:0.15015244019078225\n",
      "train loss:0.11308953899589698\n",
      "train loss:0.08628883053065385\n",
      "train loss:0.22896778321567918\n",
      "train loss:0.13524031138676226\n",
      "train loss:0.09215291916284048\n",
      "train loss:0.14389667475719994\n",
      "train loss:0.10293424623233989\n",
      "train loss:0.09742821322032594\n",
      "train loss:0.12573970247001945\n",
      "train loss:0.20134160320695396\n",
      "train loss:0.07831279131769428\n",
      "train loss:0.12738898830754547\n",
      "train loss:0.20537857564574438\n",
      "train loss:0.047195503474639716\n",
      "train loss:0.19875562504696564\n",
      "train loss:0.07543408562384112\n",
      "train loss:0.2009400950495837\n",
      "train loss:0.05835169073703989\n",
      "train loss:0.13977577731055166\n",
      "train loss:0.08984111698438282\n",
      "train loss:0.06918019661876644\n",
      "train loss:0.0821092039576346\n",
      "train loss:0.25063506708160505\n",
      "train loss:0.11381655123668749\n",
      "train loss:0.052833307705568405\n",
      "train loss:0.08696281661420237\n",
      "train loss:0.18680483903726458\n",
      "train loss:0.1279609291801441\n",
      "train loss:0.31524152127478394\n",
      "train loss:0.11840301527700425\n",
      "train loss:0.13883536659694634\n",
      "train loss:0.12362612630137032\n",
      "train loss:0.1037453477581738\n",
      "train loss:0.11584426253167798\n",
      "train loss:0.14067977922116398\n",
      "train loss:0.05497297332397208\n",
      "train loss:0.03324241252874759\n",
      "train loss:0.14298652813009813\n",
      "train loss:0.06914835679597102\n",
      "train loss:0.08873952671690206\n",
      "train loss:0.05484424287283922\n",
      "train loss:0.07466783591922516\n",
      "train loss:0.05796062562176398\n",
      "train loss:0.14388213201868197\n",
      "train loss:0.20387682459095374\n",
      "train loss:0.18106655509916209\n",
      "train loss:0.12678595892653738\n",
      "train loss:0.12490720776240892\n",
      "train loss:0.11356412303290096\n",
      "train loss:0.07396569283361203\n",
      "train loss:0.0870126756302857\n",
      "train loss:0.08214224672345125\n",
      "train loss:0.12732510345177078\n",
      "train loss:0.03939727381593972\n",
      "train loss:0.10585364953081988\n",
      "train loss:0.07353877237466688\n",
      "train loss:0.1648704415512858\n",
      "train loss:0.14474481273259165\n",
      "train loss:0.0797049783516983\n",
      "train loss:0.13869299252284614\n",
      "train loss:0.0684987577055249\n",
      "train loss:0.13739560813198629\n",
      "train loss:0.07761568749791438\n",
      "train loss:0.03218754089560825\n",
      "train loss:0.12746169300654547\n",
      "train loss:0.06252249535558577\n",
      "train loss:0.11342276135830151\n",
      "train loss:0.14523441184778044\n",
      "train loss:0.1672540624072478\n",
      "train loss:0.08191967769475365\n",
      "train loss:0.08655395293626521\n",
      "train loss:0.18204676520951388\n",
      "train loss:0.0941809186228792\n",
      "train loss:0.022728200093962286\n",
      "train loss:0.0899522351368291\n",
      "train loss:0.19745209600179997\n",
      "train loss:0.1611031423243249\n",
      "train loss:0.10053840714305264\n",
      "train loss:0.0761623343987514\n",
      "train loss:0.20173294808950393\n",
      "train loss:0.11093138293782934\n",
      "train loss:0.06537863491949567\n",
      "train loss:0.03784731596161874\n",
      "train loss:0.056397490428075854\n",
      "train loss:0.06180227365629474\n",
      "train loss:0.1307258540050377\n",
      "train loss:0.11852291939808843\n",
      "train loss:0.09693294797572914\n",
      "train loss:0.11637222438552587\n",
      "train loss:0.09276526924244653\n",
      "train loss:0.09613892930688697\n",
      "train loss:0.12662052785906433\n",
      "train loss:0.11411726788991772\n",
      "train loss:0.03275821979293216\n",
      "train loss:0.09394672760319704\n",
      "train loss:0.05276330909645306\n",
      "train loss:0.08040006857491377\n",
      "train loss:0.024409722232496177\n",
      "train loss:0.07091408942451138\n",
      "train loss:0.13176610388783835\n",
      "train loss:0.04299412514764229\n",
      "train loss:0.07123011731242328\n",
      "train loss:0.10994921109940696\n",
      "train loss:0.04455111788858234\n",
      "train loss:0.05407182070433223\n",
      "train loss:0.09268783377880338\n",
      "train loss:0.1048108554763971\n",
      "train loss:0.09182750482568576\n",
      "train loss:0.09406569802044273\n",
      "train loss:0.14677002439231968\n",
      "train loss:0.0983339326797511\n",
      "train loss:0.043615230907537966\n",
      "train loss:0.07982355707621083\n",
      "train loss:0.12097223706209174\n",
      "train loss:0.06370482802925215\n",
      "train loss:0.05773361781809402\n",
      "train loss:0.11724158209008076\n",
      "train loss:0.10999563175625315\n",
      "train loss:0.11414502846191255\n",
      "train loss:0.1485481197072704\n",
      "train loss:0.08822059892390463\n",
      "train loss:0.0646606333251708\n",
      "train loss:0.17104254008786593\n",
      "train loss:0.07259242803008324\n",
      "train loss:0.039506879107673636\n",
      "train loss:0.11363823638529245\n",
      "train loss:0.08432335574533147\n",
      "train loss:0.1295517492452376\n",
      "train loss:0.08899385933989787\n",
      "train loss:0.06064178367340791\n",
      "train loss:0.04347802169398833\n",
      "train loss:0.1111016479789809\n",
      "train loss:0.0797786735514046\n",
      "train loss:0.024022236692294207\n",
      "train loss:0.18132933171066445\n",
      "train loss:0.07103468521880439\n",
      "train loss:0.10736937069969621\n",
      "train loss:0.13772519192278282\n",
      "train loss:0.08860855978910478\n",
      "train loss:0.05455159787446517\n",
      "train loss:0.07662105015942725\n",
      "train loss:0.08637655285360511\n",
      "train loss:0.1581678181534488\n",
      "train loss:0.2439101969919458\n",
      "train loss:0.0683372456970135\n",
      "train loss:0.09328233636677292\n",
      "train loss:0.07659405985985496\n",
      "train loss:0.07909040058308046\n",
      "train loss:0.031669231647444104\n",
      "train loss:0.14662854139321058\n",
      "train loss:0.1037285103614964\n",
      "train loss:0.09159634629840659\n",
      "train loss:0.08522506400440764\n",
      "train loss:0.12067184697088772\n",
      "train loss:0.12711440300910792\n",
      "train loss:0.08669684327796288\n",
      "train loss:0.25776553971081684\n",
      "train loss:0.0891470337630086\n",
      "train loss:0.03661731208523083\n",
      "train loss:0.06375512368442107\n",
      "train loss:0.047571849818530146\n",
      "train loss:0.08752372557172083\n",
      "train loss:0.03262541223567264\n",
      "train loss:0.08595993809557047\n",
      "train loss:0.09490840611847765\n",
      "train loss:0.07662099844878434\n",
      "train loss:0.05215344213997372\n",
      "train loss:0.1378115012005498\n",
      "train loss:0.03925236309519213\n",
      "train loss:0.12314444174261233\n",
      "train loss:0.033741755542564776\n",
      "train loss:0.1349192684944299\n",
      "train loss:0.07120492493643486\n",
      "train loss:0.06968445530276586\n",
      "train loss:0.1193763756755397\n",
      "train loss:0.16706603451859667\n",
      "train loss:0.13361377148358675\n",
      "train loss:0.03949841886628367\n",
      "train loss:0.16613361251920092\n",
      "train loss:0.11141467246458117\n",
      "train loss:0.18966276801123447\n",
      "train loss:0.042209614957762695\n",
      "train loss:0.06697703927051178\n",
      "train loss:0.07276630627841767\n",
      "train loss:0.08269232659351253\n",
      "train loss:0.06195333099852441\n",
      "train loss:0.058227817155386115\n",
      "train loss:0.10307070076403659\n",
      "train loss:0.11089425431976889\n",
      "train loss:0.10942847568901731\n",
      "train loss:0.10664547060496306\n",
      "train loss:0.02564962811773539\n",
      "train loss:0.04002387681212859\n",
      "train loss:0.045919210324113655\n",
      "train loss:0.10257497631907915\n",
      "train loss:0.02783442951687124\n",
      "train loss:0.15387099121751097\n",
      "train loss:0.11128935663937332\n",
      "train loss:0.06115810715728287\n",
      "train loss:0.10616290496267347\n",
      "train loss:0.09833370016835463\n",
      "train loss:0.12714401198404462\n",
      "train loss:0.0711502481018863\n",
      "train loss:0.14109232646498637\n",
      "train loss:0.09405190307712967\n",
      "train loss:0.0828367203564361\n",
      "train loss:0.15082105122893105\n",
      "train loss:0.14284317389344797\n",
      "train loss:0.07441720019956313\n",
      "train loss:0.2620676171095164\n",
      "train loss:0.11150456988303845\n",
      "train loss:0.0688066193794839\n",
      "train loss:0.07371770112527237\n",
      "train loss:0.05910966438964201\n",
      "train loss:0.043839388195339835\n",
      "train loss:0.09211548994636862\n",
      "train loss:0.039687977755945666\n",
      "train loss:0.14309297591467415\n",
      "train loss:0.04567394404897847\n",
      "train loss:0.07633069586464594\n",
      "train loss:0.06378642235475285\n",
      "train loss:0.14729145187543055\n",
      "train loss:0.0599498121329592\n",
      "train loss:0.09915639062722288\n",
      "train loss:0.19800543557135028\n",
      "train loss:0.10655485912321097\n",
      "train loss:0.09211031400895241\n",
      "train loss:0.16332074721784434\n",
      "train loss:0.08160415128356177\n",
      "train loss:0.03703662900549157\n",
      "train loss:0.12912670253694083\n",
      "train loss:0.1193278887938758\n",
      "train loss:0.052316311799909636\n",
      "train loss:0.07856464843881233\n",
      "train loss:0.19795025916737616\n",
      "train loss:0.0358049893018004\n",
      "train loss:0.0495608034022914\n",
      "train loss:0.07157618261830521\n",
      "train loss:0.11953760975708713\n",
      "train loss:0.09931427485457309\n",
      "train loss:0.036949313653316954\n",
      "train loss:0.09585568776969022\n",
      "train loss:0.0951019717225547\n",
      "train loss:0.07643954528887577\n",
      "train loss:0.06172160836316433\n",
      "train loss:0.10710789126957915\n",
      "train loss:0.08588473045578442\n",
      "train loss:0.09083613775360476\n",
      "train loss:0.07366077546256196\n",
      "train loss:0.10606108924955569\n",
      "train loss:0.061319005124652354\n",
      "train loss:0.09549191074571034\n",
      "train loss:0.1767047807203358\n",
      "train loss:0.054985554161155005\n",
      "train loss:0.1217663394086864\n",
      "train loss:0.1406552136113925\n",
      "train loss:0.0946897753277471\n",
      "train loss:0.0650790483980036\n",
      "train loss:0.054343487165783605\n",
      "train loss:0.096998567119275\n",
      "train loss:0.24769374311790604\n",
      "train loss:0.07118555369590604\n",
      "train loss:0.03490515466605268\n",
      "train loss:0.09144673377045455\n",
      "train loss:0.06249998449131594\n",
      "train loss:0.06488072574541458\n",
      "train loss:0.05200683016039795\n",
      "train loss:0.19101635926755176\n",
      "train loss:0.08647949655362024\n",
      "train loss:0.21143881028838227\n",
      "train loss:0.054634206307238406\n",
      "train loss:0.14000175417188326\n",
      "train loss:0.07905114371594077\n",
      "train loss:0.11017896159600764\n",
      "train loss:0.10703855556309884\n",
      "train loss:0.09062598576679655\n",
      "train loss:0.10531907016575776\n",
      "train loss:0.04572339503118071\n",
      "train loss:0.06184767596237787\n",
      "train loss:0.17786009789249754\n",
      "train loss:0.05195584400354123\n",
      "train loss:0.14103591965122658\n",
      "train loss:0.049969832244715094\n",
      "train loss:0.05995718893442136\n",
      "train loss:0.06808934365161616\n",
      "train loss:0.05340445630359164\n",
      "train loss:0.06008244540419213\n",
      "train loss:0.1172588673457543\n",
      "train loss:0.04295373032282825\n",
      "train loss:0.09073908182748808\n",
      "train loss:0.0766294623215108\n",
      "train loss:0.14833443198367852\n",
      "train loss:0.09499125939926194\n",
      "train loss:0.07808763567974107\n",
      "train loss:0.09319529763526621\n",
      "train loss:0.13487782575569654\n",
      "train loss:0.06609368985927197\n",
      "train loss:0.1138281152029812\n",
      "train loss:0.04853427767081372\n",
      "train loss:0.05895300575081\n",
      "train loss:0.07411025200951701\n",
      "train loss:0.05323848110285816\n",
      "train loss:0.12247417945320167\n",
      "train loss:0.022297670840938325\n",
      "train loss:0.0479522455608799\n",
      "train loss:0.05609850473732236\n",
      "train loss:0.050819678594451904\n",
      "train loss:0.07436481358801929\n",
      "train loss:0.05339613652119876\n",
      "train loss:0.06998861302345112\n",
      "train loss:0.07683921199886899\n",
      "train loss:0.09324013708068106\n",
      "train loss:0.09686234579799276\n",
      "train loss:0.07324496656733691\n",
      "train loss:0.09480421607151289\n",
      "train loss:0.08005806773814522\n",
      "train loss:0.03875785252289292\n",
      "train loss:0.0447309226026002\n",
      "train loss:0.1030195298956114\n",
      "train loss:0.07333987488589763\n",
      "train loss:0.028860942421068243\n",
      "train loss:0.07647445826728712\n",
      "train loss:0.0443453710671088\n",
      "train loss:0.18486218283681982\n",
      "train loss:0.07174373583853955\n",
      "train loss:0.11078223326832172\n",
      "train loss:0.04345008828894874\n",
      "train loss:0.029874517620664794\n",
      "train loss:0.04774065627085003\n",
      "train loss:0.06901766852028872\n",
      "train loss:0.18918662835384492\n",
      "train loss:0.04321245989677862\n",
      "train loss:0.17331791695776194\n",
      "train loss:0.0664843949117356\n",
      "train loss:0.06213917316055565\n",
      "train loss:0.04800424743295626\n",
      "train loss:0.05391343128394669\n",
      "train loss:0.07844160179540242\n",
      "train loss:0.07350443509578528\n",
      "train loss:0.10041866103817883\n",
      "train loss:0.07468437573872927\n",
      "train loss:0.3153488147696586\n",
      "train loss:0.07655541323662536\n",
      "train loss:0.16106442140509838\n",
      "train loss:0.03103781225039637\n",
      "train loss:0.10955291412534753\n",
      "train loss:0.09588535412624631\n",
      "train loss:0.07218410777770627\n",
      "train loss:0.08442913440888818\n",
      "train loss:0.07760727189266724\n",
      "train loss:0.05500456285631947\n",
      "train loss:0.06341049714094414\n",
      "train loss:0.10888567737262936\n",
      "train loss:0.1255104140600257\n",
      "train loss:0.0674127992547419\n",
      "train loss:0.10137124505390252\n",
      "train loss:0.12172574561061136\n",
      "train loss:0.06607456235653202\n",
      "train loss:0.04595312342582243\n",
      "train loss:0.05941680171554297\n",
      "train loss:0.042549847538249815\n",
      "train loss:0.03692183469667144\n",
      "train loss:0.09018121140525945\n",
      "train loss:0.10134557201362479\n",
      "train loss:0.07940816910128726\n",
      "train loss:0.059974383789675526\n",
      "train loss:0.04355048654597647\n",
      "train loss:0.04733451851272059\n",
      "train loss:0.1647365399241202\n",
      "train loss:0.060079791886440484\n",
      "train loss:0.0631305308202807\n",
      "train loss:0.059686286748783096\n",
      "train loss:0.07490580548687889\n",
      "train loss:0.035030963191824284\n",
      "train loss:0.10322467140511478\n",
      "train loss:0.10643402044342118\n",
      "train loss:0.09676474949794008\n",
      "train loss:0.12854277486224316\n",
      "train loss:0.04964284828213522\n",
      "train loss:0.10680867055304465\n",
      "train loss:0.10097620930443388\n",
      "train loss:0.037787075526372275\n",
      "train loss:0.08463914020737316\n",
      "train loss:0.09148303920219383\n",
      "train loss:0.08627820228101568\n",
      "train loss:0.06959223256869382\n",
      "train loss:0.09382007474268539\n",
      "train loss:0.05333867301748427\n",
      "train loss:0.06928989697892365\n",
      "train loss:0.10845605946333131\n",
      "train loss:0.030618115441769955\n",
      "train loss:0.15224571709290538\n",
      "train loss:0.052198120061602626\n",
      "train loss:0.05533272395681452\n",
      "train loss:0.028249630939135895\n",
      "train loss:0.035494657397780946\n",
      "train loss:0.029368363958441626\n",
      "train loss:0.03209898810118796\n",
      "train loss:0.10744073734493488\n",
      "train loss:0.12325159051825951\n",
      "train loss:0.040060394614548446\n",
      "train loss:0.0668137972588668\n",
      "train loss:0.11877137697699071\n",
      "train loss:0.03778213726373339\n",
      "train loss:0.044108101995416454\n",
      "train loss:0.08900561002415594\n",
      "train loss:0.08461759209266356\n",
      "train loss:0.040473210680417634\n",
      "train loss:0.0713771627964455\n",
      "train loss:0.03536742072402625\n",
      "train loss:0.0653374712660762\n",
      "train loss:0.04798544771654182\n",
      "train loss:0.053482093301782066\n",
      "train loss:0.08006074263999753\n",
      "train loss:0.11202761437799187\n",
      "train loss:0.07699011338366318\n",
      "train loss:0.0757831495872162\n",
      "train loss:0.04594124759453572\n",
      "train loss:0.050480387549895686\n",
      "train loss:0.07462090005839205\n",
      "train loss:0.11213678176266606\n",
      "train loss:0.10968901733676624\n",
      "train loss:0.052342690686853335\n",
      "train loss:0.08990614459166883\n",
      "train loss:0.032138072350607484\n",
      "train loss:0.055470420325936326\n",
      "train loss:0.08624130721816925\n",
      "train loss:0.022398487140173843\n",
      "train loss:0.017809700894017483\n",
      "train loss:0.0366956296717409\n",
      "train loss:0.11468516048393938\n",
      "train loss:0.0498376921472819\n",
      "train loss:0.03789410216515936\n",
      "train loss:0.030935605655183444\n",
      "train loss:0.026075597586742893\n",
      "train loss:0.05417893995888276\n",
      "train loss:0.07438778266391273\n",
      "train loss:0.041234907003687746\n",
      "train loss:0.06241291962033662\n",
      "train loss:0.05564583286556525\n",
      "train loss:0.042142514894481585\n",
      "train loss:0.06607753256577366\n",
      "train loss:0.07001633574129683\n",
      "train loss:0.12144504475237788\n",
      "train loss:0.05257963323788136\n",
      "train loss:0.10561327070712681\n",
      "train loss:0.04720539379873869\n",
      "train loss:0.026928913205342634\n",
      "train loss:0.13363232803644418\n",
      "train loss:0.08778104018932685\n",
      "train loss:0.162769916071493\n",
      "train loss:0.07913326630368218\n",
      "train loss:0.05106570313701631\n",
      "train loss:0.09159443842751948\n",
      "train loss:0.021327326028148824\n",
      "train loss:0.195399527639279\n",
      "train loss:0.113564658010368\n",
      "train loss:0.04502746554670125\n",
      "train loss:0.07030811722572512\n",
      "train loss:0.0669891212248447\n",
      "train loss:0.04589845711112291\n",
      "train loss:0.04150571102674751\n",
      "train loss:0.1228712451705187\n",
      "train loss:0.05287645146595572\n",
      "train loss:0.06560421247375256\n",
      "train loss:0.05538269748378955\n",
      "train loss:0.14711956107876462\n",
      "train loss:0.06433362149967543\n",
      "=== epoch:3, train acc:0.976, test acc:0.971 ===\n",
      "train loss:0.13322024237966823\n",
      "train loss:0.05508506385477241\n",
      "train loss:0.09120957546519153\n",
      "train loss:0.036525489022249925\n",
      "train loss:0.03351997459147262\n",
      "train loss:0.04094805934610725\n",
      "train loss:0.08252365058681718\n",
      "train loss:0.05876371467897096\n",
      "train loss:0.04332776417551716\n",
      "train loss:0.06791675453772222\n",
      "train loss:0.0631614468389107\n",
      "train loss:0.02586764745511986\n",
      "train loss:0.09434625538421075\n",
      "train loss:0.10002480495892897\n",
      "train loss:0.022902791389766315\n",
      "train loss:0.10891967304590439\n",
      "train loss:0.06302071910968768\n",
      "train loss:0.09060068735360746\n",
      "train loss:0.059050206289861115\n",
      "train loss:0.05300420899633929\n",
      "train loss:0.09413030958833105\n",
      "train loss:0.08558610827287155\n",
      "train loss:0.040053227237234985\n",
      "train loss:0.09531260005556381\n",
      "train loss:0.03437462687028823\n",
      "train loss:0.05146994993931336\n",
      "train loss:0.08537624737737369\n",
      "train loss:0.047116303163333206\n",
      "train loss:0.10365663640794129\n",
      "train loss:0.09720004179799094\n",
      "train loss:0.0558922290345371\n",
      "train loss:0.08630481862418327\n",
      "train loss:0.07784650463787691\n",
      "train loss:0.10867764706522504\n",
      "train loss:0.03500944614225084\n",
      "train loss:0.08538696723909482\n",
      "train loss:0.05394353453224191\n",
      "train loss:0.09208299454977706\n",
      "train loss:0.05293564122387839\n",
      "train loss:0.10120234823952019\n",
      "train loss:0.06784862904754113\n",
      "train loss:0.07685866760939002\n",
      "train loss:0.08442521618013417\n",
      "train loss:0.09409486287583457\n",
      "train loss:0.06565614921919743\n",
      "train loss:0.09297147946130807\n",
      "train loss:0.044400474101496806\n",
      "train loss:0.10307382058134969\n",
      "train loss:0.044879989261078765\n",
      "train loss:0.0913020734561693\n",
      "train loss:0.045123580244599656\n",
      "train loss:0.08614959496952963\n",
      "train loss:0.1236949997792986\n",
      "train loss:0.057929201419645436\n",
      "train loss:0.059638700089250554\n",
      "train loss:0.026242800639469363\n",
      "train loss:0.05428717549631053\n",
      "train loss:0.09457141207895711\n",
      "train loss:0.07502435342848114\n",
      "train loss:0.0531114362498845\n",
      "train loss:0.07855146972608544\n",
      "train loss:0.12641617451178577\n",
      "train loss:0.08019549608950596\n",
      "train loss:0.026335949131729035\n",
      "train loss:0.08418030420235345\n",
      "train loss:0.09588186462165567\n",
      "train loss:0.10649217031390494\n",
      "train loss:0.2293411303705035\n",
      "train loss:0.031086887249274095\n",
      "train loss:0.09562935457045439\n",
      "train loss:0.10318656795603598\n",
      "train loss:0.036804792160776034\n",
      "train loss:0.05194077397488892\n",
      "train loss:0.14011720579454479\n",
      "train loss:0.05070337280253914\n",
      "train loss:0.09768933662591933\n",
      "train loss:0.06270356001697812\n",
      "train loss:0.0503155197967001\n",
      "train loss:0.052684442119989947\n",
      "train loss:0.0878821316707261\n",
      "train loss:0.049302682653810666\n",
      "train loss:0.03423454471583666\n",
      "train loss:0.04977210074102441\n",
      "train loss:0.06483267122864941\n",
      "train loss:0.04948881845472464\n",
      "train loss:0.051153853475250914\n",
      "train loss:0.11143586390221255\n",
      "train loss:0.044692546696670464\n",
      "train loss:0.028857158948243523\n",
      "train loss:0.05608356391639508\n",
      "train loss:0.06346882308663723\n",
      "train loss:0.029448495916910615\n",
      "train loss:0.05304199824742056\n",
      "train loss:0.06047374354358412\n",
      "train loss:0.031872323180210486\n",
      "train loss:0.023484299299732808\n",
      "train loss:0.07814128366043911\n",
      "train loss:0.07594517087777373\n",
      "train loss:0.009918199153916521\n",
      "train loss:0.04992459346209974\n",
      "train loss:0.06119627659609928\n",
      "train loss:0.02138117468521323\n",
      "train loss:0.0511281631615073\n",
      "train loss:0.03542236860843631\n",
      "train loss:0.05182316554436264\n",
      "train loss:0.021241214054414228\n",
      "train loss:0.02441948966486596\n",
      "train loss:0.05151861700926394\n",
      "train loss:0.04510688127707826\n",
      "train loss:0.06142066197778899\n",
      "train loss:0.0617468012478837\n",
      "train loss:0.08639137529012009\n",
      "train loss:0.05073560935376132\n",
      "train loss:0.04798150273670212\n",
      "train loss:0.044202896850377336\n",
      "train loss:0.075532897216327\n",
      "train loss:0.08443557335781303\n",
      "train loss:0.060960803095943296\n",
      "train loss:0.04698906857465775\n",
      "train loss:0.14601828790655216\n",
      "train loss:0.05753619713611387\n",
      "train loss:0.024224502246708482\n",
      "train loss:0.13545230636543212\n",
      "train loss:0.09058870150477173\n",
      "train loss:0.05771344520291051\n",
      "train loss:0.049956523312785196\n",
      "train loss:0.026284759001115106\n",
      "train loss:0.08248635355978451\n",
      "train loss:0.06702722414668154\n",
      "train loss:0.11219751679869427\n",
      "train loss:0.03193851779336703\n",
      "train loss:0.03736797659216742\n",
      "train loss:0.030348012169828124\n",
      "train loss:0.09167762970176291\n",
      "train loss:0.03468394701194337\n",
      "train loss:0.043314684806171094\n",
      "train loss:0.028591445891838773\n",
      "train loss:0.050678573000092424\n",
      "train loss:0.09306453725526465\n",
      "train loss:0.0751594592508966\n",
      "train loss:0.03930873693419059\n",
      "train loss:0.06835993385586982\n",
      "train loss:0.024181608002153104\n",
      "train loss:0.0796379810426117\n",
      "train loss:0.038237466255828696\n",
      "train loss:0.04697102943156683\n",
      "train loss:0.08448206514397016\n",
      "train loss:0.07550342108678114\n",
      "train loss:0.04774961486177876\n",
      "train loss:0.025414338670024947\n",
      "train loss:0.0335161566454124\n",
      "train loss:0.032632203842953074\n",
      "train loss:0.14044353368198972\n",
      "train loss:0.05859427578984329\n",
      "train loss:0.04529717167999539\n",
      "train loss:0.04090548198544287\n",
      "train loss:0.07474416693080684\n",
      "train loss:0.13412829938594054\n",
      "train loss:0.02866756270592672\n",
      "train loss:0.01760600739204305\n",
      "train loss:0.038720439186803395\n",
      "train loss:0.060557895963297546\n",
      "train loss:0.044092074181418396\n",
      "train loss:0.11549091208852592\n",
      "train loss:0.08138545463433781\n",
      "train loss:0.06353542579408482\n",
      "train loss:0.12936174125028213\n",
      "train loss:0.09610958224528782\n",
      "train loss:0.13405246429948878\n",
      "train loss:0.10090181621629654\n",
      "train loss:0.12555985826123534\n",
      "train loss:0.09249278305692593\n",
      "train loss:0.016303188298784382\n",
      "train loss:0.06352676312149472\n",
      "train loss:0.13530409028116192\n",
      "train loss:0.05617995718302074\n",
      "train loss:0.09802290100793647\n",
      "train loss:0.05187772352132935\n",
      "train loss:0.04900925652559774\n",
      "train loss:0.10473599751368864\n",
      "train loss:0.016092345439656777\n",
      "train loss:0.09101849617263344\n",
      "train loss:0.04489227934464173\n",
      "train loss:0.04123425836856716\n",
      "train loss:0.0757669600609007\n",
      "train loss:0.03868660321703571\n",
      "train loss:0.07152487839794273\n",
      "train loss:0.06725664222538433\n",
      "train loss:0.06772879960166017\n",
      "train loss:0.011779443539478183\n",
      "train loss:0.10347631283670346\n",
      "train loss:0.03736404956859067\n",
      "train loss:0.07458607146853631\n",
      "train loss:0.11849019750303098\n",
      "train loss:0.05820240444096085\n",
      "train loss:0.036409755098170266\n",
      "train loss:0.03103302480815264\n",
      "train loss:0.08414870613885578\n",
      "train loss:0.07714047919278755\n",
      "train loss:0.07715331464304417\n",
      "train loss:0.1322126142848661\n",
      "train loss:0.027674987540836797\n",
      "train loss:0.04608791521090429\n",
      "train loss:0.05178738520069258\n",
      "train loss:0.03116840598398188\n",
      "train loss:0.07547798339990008\n",
      "train loss:0.0378110543047321\n",
      "train loss:0.05244406019578264\n",
      "train loss:0.03134896720765286\n",
      "train loss:0.030534934349140917\n",
      "train loss:0.02935789896978357\n",
      "train loss:0.038976380647817985\n",
      "train loss:0.04605035352349161\n",
      "train loss:0.05910383501093256\n",
      "train loss:0.10636788697857845\n",
      "train loss:0.04654991456713311\n",
      "train loss:0.0752244276948364\n",
      "train loss:0.03271682556685575\n",
      "train loss:0.021342736470239237\n",
      "train loss:0.021537710618336182\n",
      "train loss:0.04675531224815389\n",
      "train loss:0.02927236737816485\n",
      "train loss:0.0376971492560157\n",
      "train loss:0.08962909987213102\n",
      "train loss:0.06543544778989316\n",
      "train loss:0.056502324995011\n",
      "train loss:0.08519442549385038\n",
      "train loss:0.032848533358811736\n",
      "train loss:0.03570684194145188\n",
      "train loss:0.0842572334373993\n",
      "train loss:0.08568786304615184\n",
      "train loss:0.09366632456938749\n",
      "train loss:0.08659809156984945\n",
      "train loss:0.06592246960178035\n",
      "train loss:0.04996586063877168\n",
      "train loss:0.0677665275603601\n",
      "train loss:0.07101423753983033\n",
      "train loss:0.10336535754675841\n",
      "train loss:0.039839420676837996\n",
      "train loss:0.07195591777518309\n",
      "train loss:0.08963558258003998\n",
      "train loss:0.12431713083957696\n",
      "train loss:0.04229588912627347\n",
      "train loss:0.03868808476709638\n",
      "train loss:0.014718830394889741\n",
      "train loss:0.052155908904078965\n",
      "train loss:0.023840490632424386\n",
      "train loss:0.04838829649487028\n",
      "train loss:0.07165287176918202\n",
      "train loss:0.05407279247490826\n",
      "train loss:0.04855035721958693\n",
      "train loss:0.04827229684748103\n",
      "train loss:0.04825773162506759\n",
      "train loss:0.013862346071355088\n",
      "train loss:0.05273775743446473\n",
      "train loss:0.04392299568970474\n",
      "train loss:0.03958357981211652\n",
      "train loss:0.07132330518890227\n",
      "train loss:0.032815485737748576\n",
      "train loss:0.10649462394641442\n",
      "train loss:0.036236499920254565\n",
      "train loss:0.09866803244770563\n",
      "train loss:0.03648355114645176\n",
      "train loss:0.03028938875542436\n",
      "train loss:0.04966819948038321\n",
      "train loss:0.01807528546092828\n",
      "train loss:0.02369207155229066\n",
      "train loss:0.02489061515438229\n",
      "train loss:0.04038922127142122\n",
      "train loss:0.050355894162297105\n",
      "train loss:0.042506281470393786\n",
      "train loss:0.08265848175362368\n",
      "train loss:0.05644820121949695\n",
      "train loss:0.013262144277464433\n",
      "train loss:0.06872176217623524\n",
      "train loss:0.0989776819563599\n",
      "train loss:0.08187687487844669\n",
      "train loss:0.05427011613863037\n",
      "train loss:0.03019198639664308\n",
      "train loss:0.029446487810288146\n",
      "train loss:0.028195485002623207\n",
      "train loss:0.06707114523376222\n",
      "train loss:0.04895030899311414\n",
      "train loss:0.0055644769686336555\n",
      "train loss:0.06302307665475705\n",
      "train loss:0.06475230711997434\n",
      "train loss:0.05732206718904254\n",
      "train loss:0.10744785863891312\n",
      "train loss:0.011783541793054204\n",
      "train loss:0.07111297368252255\n",
      "train loss:0.15371969904302088\n",
      "train loss:0.022967944807969928\n",
      "train loss:0.06275934343034764\n",
      "train loss:0.06361192413746011\n",
      "train loss:0.062380018472114676\n",
      "train loss:0.015407490511125187\n",
      "train loss:0.08248536330535507\n",
      "train loss:0.041896918503349145\n",
      "train loss:0.060847090315899056\n",
      "train loss:0.0352358511992168\n",
      "train loss:0.06171743364894572\n",
      "train loss:0.05951946275663709\n",
      "train loss:0.05681537776512116\n",
      "train loss:0.059053227504566194\n",
      "train loss:0.024750664883350298\n",
      "train loss:0.02086855244910698\n",
      "train loss:0.06045303911572977\n",
      "train loss:0.042008307310672406\n",
      "train loss:0.028157315395913755\n",
      "train loss:0.026352579678022018\n",
      "train loss:0.03585393500576792\n",
      "train loss:0.03776570266977055\n",
      "train loss:0.057287036937428225\n",
      "train loss:0.014977603126655274\n",
      "train loss:0.03385984787511146\n",
      "train loss:0.15786945544060763\n",
      "train loss:0.03204076877145183\n",
      "train loss:0.01822238110735331\n",
      "train loss:0.10235209358355543\n",
      "train loss:0.04498956947023736\n",
      "train loss:0.04644166789549317\n",
      "train loss:0.02977837995618352\n",
      "train loss:0.075405160422702\n",
      "train loss:0.03309281545455858\n",
      "train loss:0.019982106648039562\n",
      "train loss:0.04902444673170204\n",
      "train loss:0.018269954229599393\n",
      "train loss:0.08492224449464836\n",
      "train loss:0.034277452582547135\n",
      "train loss:0.07543021244499387\n",
      "train loss:0.029495434225885166\n",
      "train loss:0.06537098435027783\n",
      "train loss:0.09986914632612828\n",
      "train loss:0.042942580500362544\n",
      "train loss:0.07322357436765177\n",
      "train loss:0.015378647162663068\n",
      "train loss:0.04926872382923242\n",
      "train loss:0.039745078074332446\n",
      "train loss:0.06711219110511635\n",
      "train loss:0.020847484164583516\n",
      "train loss:0.0629424491803513\n",
      "train loss:0.031774535184403846\n",
      "train loss:0.038388539868015055\n",
      "train loss:0.054704089698205435\n",
      "train loss:0.04105163176307898\n",
      "train loss:0.07300884940000628\n",
      "train loss:0.05872299612454829\n",
      "train loss:0.02979216109131076\n",
      "train loss:0.013510194134574615\n",
      "train loss:0.053858089002085156\n",
      "train loss:0.046534447087066534\n",
      "train loss:0.05999372497561909\n",
      "train loss:0.030887284510313494\n",
      "train loss:0.051843672484352225\n",
      "train loss:0.07907091799477492\n",
      "train loss:0.04222888266233511\n",
      "train loss:0.033123076856552476\n",
      "train loss:0.08080069480595743\n",
      "train loss:0.08468537020862028\n",
      "train loss:0.05537683073980322\n",
      "train loss:0.011289404918294103\n",
      "train loss:0.0941190876479657\n",
      "train loss:0.11484019508979604\n",
      "train loss:0.04025431352650994\n",
      "train loss:0.022112978705902585\n",
      "train loss:0.016622319400099464\n",
      "train loss:0.0429616454049532\n",
      "train loss:0.15275689000623044\n",
      "train loss:0.10057283875172077\n",
      "train loss:0.13982575836806505\n",
      "train loss:0.018879644915980015\n",
      "train loss:0.05611411571550149\n",
      "train loss:0.012627853008399848\n",
      "train loss:0.05620416010700604\n",
      "train loss:0.041411005353638036\n",
      "train loss:0.0290989027560595\n",
      "train loss:0.02094482607305332\n",
      "train loss:0.057455529427639505\n",
      "train loss:0.031991629129316974\n",
      "train loss:0.051594086659063565\n",
      "train loss:0.050632677001276126\n",
      "train loss:0.03154236111403954\n",
      "train loss:0.03202366695982899\n",
      "train loss:0.051413344787787446\n",
      "train loss:0.07992689714154298\n",
      "train loss:0.1264141903072721\n",
      "train loss:0.008988190067037861\n",
      "train loss:0.061128160973411\n",
      "train loss:0.01749132815382475\n",
      "train loss:0.022013449988453164\n",
      "train loss:0.016208822597166637\n",
      "train loss:0.030052009570190937\n",
      "train loss:0.056481543699417304\n",
      "train loss:0.022994772674554423\n",
      "train loss:0.06787534732356051\n",
      "train loss:0.0690731204183624\n",
      "train loss:0.01425068903903758\n",
      "train loss:0.006761666974207298\n",
      "train loss:0.061178669533694524\n",
      "train loss:0.039022588294038096\n",
      "train loss:0.06637782496760834\n",
      "train loss:0.10808440683494341\n",
      "train loss:0.047051597538134954\n",
      "train loss:0.07694313167985799\n",
      "train loss:0.06212457825853626\n",
      "train loss:0.06654823483336973\n",
      "train loss:0.027813856130061838\n",
      "train loss:0.030968385760027785\n",
      "train loss:0.039279019050634884\n",
      "train loss:0.09748304858239826\n",
      "train loss:0.028124098453424985\n",
      "train loss:0.045442866819375455\n",
      "train loss:0.016823398343332695\n",
      "train loss:0.05378089422306212\n",
      "train loss:0.020610189568039002\n",
      "train loss:0.011784540400937437\n",
      "train loss:0.045321263624364685\n",
      "train loss:0.04532751496478606\n",
      "train loss:0.014162463884970746\n",
      "train loss:0.15141600566072017\n",
      "train loss:0.04008267577888376\n",
      "train loss:0.02436254474462331\n",
      "train loss:0.02428407567808529\n",
      "train loss:0.12273322095080026\n",
      "train loss:0.022752314123300425\n",
      "train loss:0.05061673303021865\n",
      "train loss:0.03132580841411287\n",
      "train loss:0.08423235920158033\n",
      "train loss:0.054448552498695235\n",
      "train loss:0.02265734970026094\n",
      "train loss:0.014865898115540623\n",
      "train loss:0.014070287550699287\n",
      "train loss:0.06010150080575889\n",
      "train loss:0.12321568066264055\n",
      "train loss:0.03715334127443754\n",
      "train loss:0.012637158902328376\n",
      "train loss:0.04733799327760622\n",
      "train loss:0.06845994681783582\n",
      "train loss:0.02719503307609582\n",
      "train loss:0.015633940897906394\n",
      "train loss:0.026774348955158508\n",
      "train loss:0.04361455814612428\n",
      "train loss:0.010543084584530021\n",
      "train loss:0.024071890169264965\n",
      "train loss:0.051950833073664715\n",
      "train loss:0.0768473324898573\n",
      "train loss:0.048310155834508005\n",
      "train loss:0.009631782851562729\n",
      "train loss:0.023366185507038263\n",
      "train loss:0.05934456573208422\n",
      "train loss:0.08926995425776683\n",
      "train loss:0.0655984419195655\n",
      "train loss:0.10489542856171707\n",
      "train loss:0.007148146655888668\n",
      "train loss:0.025926747319549914\n",
      "train loss:0.02075207872451914\n",
      "train loss:0.041172716067538265\n",
      "train loss:0.042530889535637446\n",
      "train loss:0.062139213249104\n",
      "train loss:0.018343251859369185\n",
      "train loss:0.04181009858639387\n",
      "train loss:0.04531732157453924\n",
      "train loss:0.029589250798407976\n",
      "train loss:0.02901101105553547\n",
      "train loss:0.02464384208349635\n",
      "train loss:0.06505560326226686\n",
      "train loss:0.014201405904362143\n",
      "train loss:0.09506563713240307\n",
      "train loss:0.2365142785990976\n",
      "train loss:0.07981742422319868\n",
      "train loss:0.04228747434422294\n",
      "train loss:0.08390067896256354\n",
      "train loss:0.05170080960473972\n",
      "train loss:0.05275730326139097\n",
      "train loss:0.05229774811964656\n",
      "train loss:0.06275804501068577\n",
      "train loss:0.031012099027538022\n",
      "train loss:0.01773183819121843\n",
      "train loss:0.15018338543123402\n",
      "train loss:0.04085056356490076\n",
      "train loss:0.040517710326561804\n",
      "train loss:0.03213231714029286\n",
      "train loss:0.026761842070092044\n",
      "train loss:0.015453126887347728\n",
      "train loss:0.07576797970286438\n",
      "train loss:0.04455878192189622\n",
      "train loss:0.04663779981248193\n",
      "train loss:0.012582018703324703\n",
      "train loss:0.05841566830768623\n",
      "train loss:0.04608642446389062\n",
      "train loss:0.05766587059383382\n",
      "train loss:0.04862526750744589\n",
      "train loss:0.06358063465881966\n",
      "train loss:0.05808017532249136\n",
      "train loss:0.08835656505183768\n",
      "train loss:0.08090023771475068\n",
      "train loss:0.022882590017600037\n",
      "train loss:0.09470455564245206\n",
      "train loss:0.02500551941013632\n",
      "train loss:0.04089610246613116\n",
      "train loss:0.019216394070134892\n",
      "train loss:0.05436030770644247\n",
      "train loss:0.03606813551096004\n",
      "train loss:0.04224001944872275\n",
      "train loss:0.08229337196642117\n",
      "train loss:0.03906743745128541\n",
      "train loss:0.031499572228814395\n",
      "train loss:0.06505126912516199\n",
      "train loss:0.04936357741131179\n",
      "train loss:0.03536855341915649\n",
      "train loss:0.04615381951835767\n",
      "train loss:0.007518323932808044\n",
      "train loss:0.033444130957068066\n",
      "train loss:0.05735917477414024\n",
      "train loss:0.1323902504446647\n",
      "train loss:0.02251407218989288\n",
      "train loss:0.021429193230722537\n",
      "train loss:0.0402632039318447\n",
      "train loss:0.05858085532575317\n",
      "train loss:0.047708614435253226\n",
      "train loss:0.03013865943951074\n",
      "train loss:0.03897850063532479\n",
      "train loss:0.031243251789170784\n",
      "train loss:0.013217961697235192\n",
      "train loss:0.03786416896338626\n",
      "train loss:0.05808964982365297\n",
      "train loss:0.029936209588379212\n",
      "train loss:0.04322504696596564\n",
      "train loss:0.03524965094055422\n",
      "train loss:0.04062996605166891\n",
      "train loss:0.051417178968235736\n",
      "train loss:0.027619557427919997\n",
      "train loss:0.08429954841475551\n",
      "train loss:0.06426354444518113\n",
      "train loss:0.0690990032109912\n",
      "train loss:0.050615889050738466\n",
      "train loss:0.11695840956555577\n",
      "train loss:0.059950510374205346\n",
      "train loss:0.05492677805424437\n",
      "train loss:0.13008838930629638\n",
      "train loss:0.06745114807746376\n",
      "train loss:0.11924087483719635\n",
      "train loss:0.1055090122313615\n",
      "train loss:0.042126537109244744\n",
      "train loss:0.023683733942458107\n",
      "train loss:0.029290153926576017\n",
      "train loss:0.028832967573362178\n",
      "train loss:0.0752357430745816\n",
      "train loss:0.03976852778373561\n",
      "train loss:0.06606452761699153\n",
      "train loss:0.06269834314886351\n",
      "train loss:0.12701365867348258\n",
      "train loss:0.01983350246178846\n",
      "train loss:0.07220091924836795\n",
      "train loss:0.04306361761563248\n",
      "train loss:0.011346476748474681\n",
      "train loss:0.03982485661028658\n",
      "train loss:0.052708361179022584\n",
      "train loss:0.11018066251405877\n",
      "train loss:0.04115828836802099\n",
      "train loss:0.0173054867616519\n",
      "train loss:0.1460465358970512\n",
      "train loss:0.04316823167772909\n",
      "train loss:0.08962208586351947\n",
      "train loss:0.09097292669942289\n",
      "train loss:0.03325068854614018\n",
      "train loss:0.053917662295146565\n",
      "train loss:0.028052219430631006\n",
      "train loss:0.054031167301766075\n",
      "train loss:0.015700445817000065\n",
      "train loss:0.07711827380748824\n",
      "train loss:0.05626556898664088\n",
      "train loss:0.16065866265920925\n",
      "train loss:0.05952816956915066\n",
      "train loss:0.041277651731791126\n",
      "train loss:0.05258316886390472\n",
      "train loss:0.02395723490795464\n",
      "train loss:0.009905120166892392\n",
      "train loss:0.01502026131460641\n",
      "train loss:0.04909501947359933\n",
      "train loss:0.021036161451924728\n",
      "train loss:0.07517927987442247\n",
      "train loss:0.1919638327275802\n",
      "train loss:0.03389166509353396\n",
      "train loss:0.03721209365654277\n",
      "train loss:0.05412764854214348\n",
      "train loss:0.05490496976952849\n",
      "train loss:0.07638191923769556\n",
      "train loss:0.025361687241971503\n",
      "train loss:0.021911470138428667\n",
      "train loss:0.04204678131541924\n",
      "train loss:0.06748849834321667\n",
      "train loss:0.034033440428437686\n",
      "train loss:0.03231929594222563\n",
      "train loss:0.012382572397264275\n",
      "train loss:0.04582149650463348\n",
      "train loss:0.03176875579499603\n",
      "train loss:0.00931836613468281\n",
      "train loss:0.024240422850171205\n",
      "train loss:0.01685459397000881\n",
      "=== epoch:4, train acc:0.987, test acc:0.978 ===\n",
      "train loss:0.06455702352185362\n",
      "train loss:0.022383561067883716\n",
      "train loss:0.0662329146277982\n",
      "train loss:0.0449264375859725\n",
      "train loss:0.029188094870883786\n",
      "train loss:0.07009627306378878\n",
      "train loss:0.029197554900416926\n",
      "train loss:0.05104273305529079\n",
      "train loss:0.05443874877049959\n",
      "train loss:0.050989343990979534\n",
      "train loss:0.006330401487234183\n",
      "train loss:0.015218917465123459\n",
      "train loss:0.03514452384756928\n",
      "train loss:0.05130611582250177\n",
      "train loss:0.03571066392987735\n",
      "train loss:0.06952678419184592\n",
      "train loss:0.015886916111496542\n",
      "train loss:0.05056631776352595\n",
      "train loss:0.01592747980465543\n",
      "train loss:0.08477624996031592\n",
      "train loss:0.04971535821254247\n",
      "train loss:0.030919854766230496\n",
      "train loss:0.035876649776749765\n",
      "train loss:0.031191114230059554\n",
      "train loss:0.10958563327730357\n",
      "train loss:0.024503724927892392\n",
      "train loss:0.007734527440349015\n",
      "train loss:0.011171374760652376\n",
      "train loss:0.0282576516160572\n",
      "train loss:0.03059514486770587\n",
      "train loss:0.029205295466950516\n",
      "train loss:0.029268727994521845\n",
      "train loss:0.03404917181864842\n",
      "train loss:0.04584154497940347\n",
      "train loss:0.04273088899508076\n",
      "train loss:0.012757276403650047\n",
      "train loss:0.08208394202993194\n",
      "train loss:0.05361881249873635\n",
      "train loss:0.07835460159449678\n",
      "train loss:0.04349524893517726\n",
      "train loss:0.03352801565305935\n",
      "train loss:0.02128358791309867\n",
      "train loss:0.046516923716515925\n",
      "train loss:0.03543027686457978\n",
      "train loss:0.027295234379157565\n",
      "train loss:0.08294584772686468\n",
      "train loss:0.049811859744872525\n",
      "train loss:0.020852077355437922\n",
      "train loss:0.11552064785225205\n",
      "train loss:0.07816282781793522\n",
      "train loss:0.11679441851351506\n",
      "train loss:0.01642471674804867\n",
      "train loss:0.028863348702782662\n",
      "train loss:0.05342208571985819\n",
      "train loss:0.07904290628851078\n",
      "train loss:0.015794411632440553\n",
      "train loss:0.021306992187475792\n",
      "train loss:0.0314328223759804\n",
      "train loss:0.08961213721894375\n",
      "train loss:0.026447608039405875\n",
      "train loss:0.024069861281180886\n",
      "train loss:0.012019170821748564\n",
      "train loss:0.05316583605836921\n",
      "train loss:0.016526789810527697\n",
      "train loss:0.07060640321862824\n",
      "train loss:0.06024775177445321\n",
      "train loss:0.05626169712596023\n",
      "train loss:0.03394676802541027\n",
      "train loss:0.020117850099814774\n",
      "train loss:0.01531061280461701\n",
      "train loss:0.054039545586377705\n",
      "train loss:0.03857123284111729\n",
      "train loss:0.07352671090271114\n",
      "train loss:0.10232043834149016\n",
      "train loss:0.006240752361790519\n",
      "train loss:0.08658678399737502\n",
      "train loss:0.011255315890188165\n",
      "train loss:0.03113001650304561\n",
      "train loss:0.06334307605301441\n",
      "train loss:0.2033370066085453\n",
      "train loss:0.040950393055639064\n",
      "train loss:0.05620638573318259\n",
      "train loss:0.04699713742244473\n",
      "train loss:0.018338235114566344\n",
      "train loss:0.09670091934533369\n",
      "train loss:0.04845434691998203\n",
      "train loss:0.03973347810607204\n",
      "train loss:0.09170434125469468\n",
      "train loss:0.016716617590712732\n",
      "train loss:0.05682415716080278\n",
      "train loss:0.041509499756465985\n",
      "train loss:0.02076309659716922\n",
      "train loss:0.03499068103028574\n",
      "train loss:0.02254836273090805\n",
      "train loss:0.07865103045068227\n",
      "train loss:0.015144894013190646\n",
      "train loss:0.029336612045606766\n",
      "train loss:0.004618568912206029\n",
      "train loss:0.1288194140959752\n",
      "train loss:0.0720483329668951\n",
      "train loss:0.01695094764295665\n",
      "train loss:0.02551507699896968\n",
      "train loss:0.062110917339880105\n",
      "train loss:0.0847929783287787\n",
      "train loss:0.03972119693965027\n",
      "train loss:0.02699954678281808\n",
      "train loss:0.06378556251298458\n",
      "train loss:0.06806418828677174\n",
      "train loss:0.039785601286670934\n",
      "train loss:0.0937471209632539\n",
      "train loss:0.05856806903526923\n",
      "train loss:0.005786642766483196\n",
      "train loss:0.008379154778459486\n",
      "train loss:0.028278728888200873\n",
      "train loss:0.10101573528786977\n",
      "train loss:0.05779417552024585\n",
      "train loss:0.08122093270802581\n",
      "train loss:0.07800740678284464\n",
      "train loss:0.02801898496959751\n",
      "train loss:0.06266181553270306\n",
      "train loss:0.023409090422497784\n",
      "train loss:0.02189644780753697\n",
      "train loss:0.051755409119878824\n",
      "train loss:0.016047972637870868\n",
      "train loss:0.030501817906888995\n",
      "train loss:0.019415074997589987\n",
      "train loss:0.07289399332339017\n",
      "train loss:0.030582627204856452\n",
      "train loss:0.09177874058489356\n",
      "train loss:0.008811788453502882\n",
      "train loss:0.027123338312743786\n",
      "train loss:0.011337335513817614\n",
      "train loss:0.03424694669216668\n",
      "train loss:0.04354292725547863\n",
      "train loss:0.036739597462556624\n",
      "train loss:0.008014048699917292\n",
      "train loss:0.05054842044527935\n",
      "train loss:0.044203392338471294\n",
      "train loss:0.03444523723182193\n",
      "train loss:0.027580721997660203\n",
      "train loss:0.025465705996563948\n",
      "train loss:0.023982966023063733\n",
      "train loss:0.014745450652376247\n",
      "train loss:0.048304365177823355\n",
      "train loss:0.09219714966654376\n",
      "train loss:0.030791821353673244\n",
      "train loss:0.014641153093286407\n",
      "train loss:0.04756864666827096\n",
      "train loss:0.013185985439143611\n",
      "train loss:0.14495178768849176\n",
      "train loss:0.0179660878275235\n",
      "train loss:0.029158527214623257\n",
      "train loss:0.039775666257497726\n",
      "train loss:0.04364487364079385\n",
      "train loss:0.14783928773269847\n",
      "train loss:0.01732980109163648\n",
      "train loss:0.019106131680708176\n",
      "train loss:0.11370697770481848\n",
      "train loss:0.03328036172780214\n",
      "train loss:0.015422440555957236\n",
      "train loss:0.05532215585316888\n",
      "train loss:0.01354966464633316\n",
      "train loss:0.1020734392418859\n",
      "train loss:0.03460487220035715\n",
      "train loss:0.0076475767076774285\n",
      "train loss:0.04955752001236644\n",
      "train loss:0.04190254142621368\n",
      "train loss:0.011758207468157169\n",
      "train loss:0.03976183265976046\n",
      "train loss:0.01803214079331613\n",
      "train loss:0.11734375346125842\n",
      "train loss:0.06451677638780297\n",
      "train loss:0.03128638291111534\n",
      "train loss:0.048757506539048465\n",
      "train loss:0.04540715848721552\n",
      "train loss:0.025903935146023304\n",
      "train loss:0.016681334123136243\n",
      "train loss:0.02695441826680538\n",
      "train loss:0.053325506372583424\n",
      "train loss:0.0419862769734252\n",
      "train loss:0.03360811238782658\n",
      "train loss:0.08708797301086545\n",
      "train loss:0.04002193050805723\n",
      "train loss:0.04107998630792412\n",
      "train loss:0.03133907339440902\n",
      "train loss:0.03448774915840298\n",
      "train loss:0.05652135123166646\n",
      "train loss:0.018125376629271148\n",
      "train loss:0.06272038176185783\n",
      "train loss:0.10795163211106877\n",
      "train loss:0.025253414250232156\n",
      "train loss:0.04502785471737721\n",
      "train loss:0.043222325183115605\n",
      "train loss:0.023334648671772464\n",
      "train loss:0.05017951318284722\n",
      "train loss:0.014314269113843923\n",
      "train loss:0.020807498024457166\n",
      "train loss:0.05437148450699523\n",
      "train loss:0.022800615827775837\n",
      "train loss:0.06383066401022569\n",
      "train loss:0.012280605214486751\n",
      "train loss:0.04260490666585217\n",
      "train loss:0.04598685057766832\n",
      "train loss:0.023414333401972166\n",
      "train loss:0.07130200830110646\n",
      "train loss:0.01817662427831076\n",
      "train loss:0.0432642966749912\n",
      "train loss:0.026887256250149805\n",
      "train loss:0.05542386325672392\n",
      "train loss:0.010641303825545775\n",
      "train loss:0.0733605148951083\n",
      "train loss:0.037531266184132134\n",
      "train loss:0.04743640912417506\n",
      "train loss:0.019583759908549186\n",
      "train loss:0.04234568131491273\n",
      "train loss:0.02383225646681737\n",
      "train loss:0.04931260034948325\n",
      "train loss:0.07881291253128352\n",
      "train loss:0.029212503436188468\n",
      "train loss:0.06277409913398695\n",
      "train loss:0.02323319559739749\n",
      "train loss:0.005128922318297702\n",
      "train loss:0.021145086661069446\n",
      "train loss:0.008639063600925244\n",
      "train loss:0.02385314894797398\n",
      "train loss:0.043794193921701925\n",
      "train loss:0.03814128131288735\n",
      "train loss:0.02431171986261972\n",
      "train loss:0.06411053672879803\n",
      "train loss:0.05643522415351513\n",
      "train loss:0.025142416045446488\n",
      "train loss:0.10765317867907129\n",
      "train loss:0.021047520567431036\n",
      "train loss:0.038579714657560556\n",
      "train loss:0.04511728200984594\n",
      "train loss:0.014468160028822261\n",
      "train loss:0.015957086893756897\n",
      "train loss:0.028434054205667546\n",
      "train loss:0.021983332235665735\n",
      "train loss:0.03976704806165982\n",
      "train loss:0.010789130395216527\n",
      "train loss:0.08753083912400841\n",
      "train loss:0.05364331579085089\n",
      "train loss:0.015919378125103387\n",
      "train loss:0.07364644474452815\n",
      "train loss:0.025310074397131888\n",
      "train loss:0.037197675000209554\n",
      "train loss:0.018330316459642736\n",
      "train loss:0.01842791271891713\n",
      "train loss:0.04931781400817938\n",
      "train loss:0.015935986526275214\n",
      "train loss:0.019495441588677803\n",
      "train loss:0.03273711928081395\n",
      "train loss:0.03550450401441534\n",
      "train loss:0.021939016250324683\n",
      "train loss:0.0666613019358478\n",
      "train loss:0.009060793860819953\n",
      "train loss:0.03142703708368352\n",
      "train loss:0.038833194143846506\n",
      "train loss:0.07078544044230167\n",
      "train loss:0.025570223280338627\n",
      "train loss:0.014591877085732856\n",
      "train loss:0.023822879241610733\n",
      "train loss:0.012131482512538013\n",
      "train loss:0.1086788183924649\n",
      "train loss:0.050808482020146026\n",
      "train loss:0.01729431120413568\n",
      "train loss:0.04998434780355578\n",
      "train loss:0.09146123943262867\n",
      "train loss:0.022944877847953585\n",
      "train loss:0.03315957651095054\n",
      "train loss:0.004848552213048708\n",
      "train loss:0.014536771010792683\n",
      "train loss:0.04299698813730484\n",
      "train loss:0.028814914881242822\n",
      "train loss:0.04281107843207858\n",
      "train loss:0.03894728792882792\n",
      "train loss:0.018490571304972912\n",
      "train loss:0.09168582286323183\n",
      "train loss:0.051080541754336614\n",
      "train loss:0.012895431296868551\n",
      "train loss:0.04459264773526614\n",
      "train loss:0.012801516541552213\n",
      "train loss:0.025869442684770417\n",
      "train loss:0.013441487675085066\n",
      "train loss:0.05816817182339268\n",
      "train loss:0.06295439928222442\n",
      "train loss:0.017835891888460912\n",
      "train loss:0.040035480728686276\n",
      "train loss:0.011105477832223248\n",
      "train loss:0.03337512693417007\n",
      "train loss:0.012438795383932475\n",
      "train loss:0.055895010594377265\n",
      "train loss:0.0251998130131008\n",
      "train loss:0.02071324971453809\n",
      "train loss:0.028055688970277163\n",
      "train loss:0.023572378502264367\n",
      "train loss:0.033353089013458244\n",
      "train loss:0.011010354927581211\n",
      "train loss:0.02014257229983103\n",
      "train loss:0.03455054397182697\n",
      "train loss:0.09926489865123311\n",
      "train loss:0.038841978162691124\n",
      "train loss:0.035109576876731385\n",
      "train loss:0.01012470978874002\n",
      "train loss:0.02464967101527387\n",
      "train loss:0.08304835500352009\n",
      "train loss:0.01738694073370771\n",
      "train loss:0.02997041984722886\n",
      "train loss:0.054409935691623675\n",
      "train loss:0.017717085100337385\n",
      "train loss:0.024014592377500317\n",
      "train loss:0.025599720265280984\n",
      "train loss:0.06017889843765422\n",
      "train loss:0.03220740763700113\n",
      "train loss:0.03931259521438829\n",
      "train loss:0.03183532833054182\n",
      "train loss:0.06900795667735121\n",
      "train loss:0.01301093396551376\n",
      "train loss:0.011633314861821604\n",
      "train loss:0.030625549248073366\n",
      "train loss:0.04046633900836434\n",
      "train loss:0.02155924899240291\n",
      "train loss:0.015702528540559965\n",
      "train loss:0.024221669654412654\n",
      "train loss:0.018444572064788622\n",
      "train loss:0.03684026642174769\n",
      "train loss:0.06708028522282494\n",
      "train loss:0.054517178572174224\n",
      "train loss:0.003453279154520564\n",
      "train loss:0.015193816772670548\n",
      "train loss:0.013034557596200147\n",
      "train loss:0.08411174195987835\n",
      "train loss:0.01611992740179393\n",
      "train loss:0.04244035493019472\n",
      "train loss:0.01252663030422282\n",
      "train loss:0.025134613753501688\n",
      "train loss:0.11565639731692469\n",
      "train loss:0.02091726472514629\n",
      "train loss:0.032526545454143616\n",
      "train loss:0.029874029758725662\n",
      "train loss:0.03945705189282172\n",
      "train loss:0.02217874021352476\n",
      "train loss:0.07782102854985042\n",
      "train loss:0.03623141293359017\n",
      "train loss:0.04143382743726701\n",
      "train loss:0.04824400383663375\n",
      "train loss:0.02054578849408216\n",
      "train loss:0.015279453358404676\n",
      "train loss:0.024152188712335772\n",
      "train loss:0.05524726258978421\n",
      "train loss:0.030799801951032237\n",
      "train loss:0.007524046318852859\n",
      "train loss:0.017836094914495282\n",
      "train loss:0.03085263345973229\n",
      "train loss:0.02961353261794137\n",
      "train loss:0.057462086341081874\n",
      "train loss:0.12720825016420587\n",
      "train loss:0.013031710074218\n",
      "train loss:0.028570436698496827\n",
      "train loss:0.09055802020518869\n",
      "train loss:0.024343355200864683\n",
      "train loss:0.02167689177208275\n",
      "train loss:0.09859611699128866\n",
      "train loss:0.012422167265027383\n",
      "train loss:0.07536306393639283\n",
      "train loss:0.024614373042677978\n",
      "train loss:0.011659553692602558\n",
      "train loss:0.01891102380403616\n",
      "train loss:0.02076070475795474\n",
      "train loss:0.053325199182527566\n",
      "train loss:0.04191599449460355\n",
      "train loss:0.023415822564515878\n",
      "train loss:0.03617779345617214\n",
      "train loss:0.02893387382362789\n",
      "train loss:0.059329953333055255\n",
      "train loss:0.08304528976755492\n",
      "train loss:0.01672197264081337\n",
      "train loss:0.029514045151504832\n",
      "train loss:0.03484113568228621\n",
      "train loss:0.04620294315196803\n",
      "train loss:0.049195328283234314\n",
      "train loss:0.04936806291282134\n",
      "train loss:0.022040512181143158\n",
      "train loss:0.008631172727706679\n",
      "train loss:0.02275166310250261\n",
      "train loss:0.018068599989761118\n",
      "train loss:0.01910395214492753\n",
      "train loss:0.046797855091583636\n",
      "train loss:0.04746046498063902\n",
      "train loss:0.018071906300585725\n",
      "train loss:0.054205206064453\n",
      "train loss:0.015711994058168272\n",
      "train loss:0.04334384058920419\n",
      "train loss:0.01512093867807741\n",
      "train loss:0.0173426073556405\n",
      "train loss:0.0246688678735258\n",
      "train loss:0.010960129335403225\n",
      "train loss:0.02411352634695476\n",
      "train loss:0.0437542243624373\n",
      "train loss:0.07723183961715802\n",
      "train loss:0.029888918219019144\n",
      "train loss:0.017956544358542558\n",
      "train loss:0.009196089313163425\n",
      "train loss:0.046457437757820415\n",
      "train loss:0.013643415760248731\n",
      "train loss:0.054548909501467874\n",
      "train loss:0.06286242534462505\n",
      "train loss:0.018898498510653693\n",
      "train loss:0.05299628443059575\n",
      "train loss:0.04043586127268733\n",
      "train loss:0.03599188370740888\n",
      "train loss:0.009927060547435711\n",
      "train loss:0.03656125477846139\n",
      "train loss:0.014538274464755405\n",
      "train loss:0.05276940459430313\n",
      "train loss:0.0606483573460861\n",
      "train loss:0.05037907360820522\n",
      "train loss:0.027682902890843613\n",
      "train loss:0.071765157268505\n",
      "train loss:0.08271595302652898\n",
      "train loss:0.01806472588652653\n",
      "train loss:0.08793105523464328\n",
      "train loss:0.03184303430414044\n",
      "train loss:0.09555699245599701\n",
      "train loss:0.030889888483863728\n",
      "train loss:0.04982585065251929\n",
      "train loss:0.09378245480079077\n",
      "train loss:0.010397578355666905\n",
      "train loss:0.022163339878631574\n",
      "train loss:0.021744460954510946\n",
      "train loss:0.05022526548835659\n",
      "train loss:0.012980678301754011\n",
      "train loss:0.030094275650373814\n",
      "train loss:0.11448422280896726\n",
      "train loss:0.017217038766571543\n",
      "train loss:0.040209562289097084\n",
      "train loss:0.01712389839756322\n",
      "train loss:0.014842584033586757\n",
      "train loss:0.010751766625562303\n",
      "train loss:0.0195337219132049\n",
      "train loss:0.038948515970786475\n",
      "train loss:0.001909806726028626\n",
      "train loss:0.05256472795105441\n",
      "train loss:0.010347001512329021\n",
      "train loss:0.027469834672675927\n",
      "train loss:0.04348596888594978\n",
      "train loss:0.07573128711016971\n",
      "train loss:0.026064690712728326\n",
      "train loss:0.017493175756688167\n",
      "train loss:0.019956945996936284\n",
      "train loss:0.039768990053436676\n",
      "train loss:0.042692053941864316\n",
      "train loss:0.0561572381137639\n",
      "train loss:0.0739444981686292\n",
      "train loss:0.013580349882910321\n",
      "train loss:0.039745331080073595\n",
      "train loss:0.02971309580992652\n",
      "train loss:0.00968495265121302\n",
      "train loss:0.023649759294004327\n",
      "train loss:0.00785976648430226\n",
      "train loss:0.0820938672597201\n",
      "train loss:0.021495331495626106\n",
      "train loss:0.06999809340521267\n",
      "train loss:0.02712401134553664\n",
      "train loss:0.051380939488847234\n",
      "train loss:0.0338245340553834\n",
      "train loss:0.02272094447981017\n",
      "train loss:0.06893302127586115\n",
      "train loss:0.0780238702073329\n",
      "train loss:0.079312655996333\n",
      "train loss:0.007581481834258405\n",
      "train loss:0.011824883548351747\n",
      "train loss:0.013359753884956525\n",
      "train loss:0.033839549008942746\n",
      "train loss:0.023721000105223742\n",
      "train loss:0.06111758953803683\n",
      "train loss:0.01114027826087331\n",
      "train loss:0.05446761176293344\n",
      "train loss:0.07590411788247228\n",
      "train loss:0.026565564615484247\n",
      "train loss:0.018611524168813814\n",
      "train loss:0.017603216788964048\n",
      "train loss:0.0120760460408588\n",
      "train loss:0.06738431866440946\n",
      "train loss:0.06170392602159379\n",
      "train loss:0.01718627448574177\n",
      "train loss:0.03083517814537924\n",
      "train loss:0.03927040439084744\n",
      "train loss:0.008444727076866686\n",
      "train loss:0.06108476080621914\n",
      "train loss:0.0072240254187474874\n",
      "train loss:0.03885766645844352\n",
      "train loss:0.06609214527957186\n",
      "train loss:0.018691792317798793\n",
      "train loss:0.01621659197624389\n",
      "train loss:0.03696983002253435\n",
      "train loss:0.04911747341223423\n",
      "train loss:0.01272020425029004\n",
      "train loss:0.057173367844737394\n",
      "train loss:0.008996361623964815\n",
      "train loss:0.02079046557806981\n",
      "train loss:0.03689427807910696\n",
      "train loss:0.014136817190258076\n",
      "train loss:0.013760017479940352\n",
      "train loss:0.03511305013412584\n",
      "train loss:0.06578703031892003\n",
      "train loss:0.0793346073977119\n",
      "train loss:0.05720108638478079\n",
      "train loss:0.033728773545355896\n",
      "train loss:0.017119057863455342\n",
      "train loss:0.0486081437486659\n",
      "train loss:0.1428630665382527\n",
      "train loss:0.019673166781928268\n",
      "train loss:0.021705305953587675\n",
      "train loss:0.15317959196460604\n",
      "train loss:0.04068978986240375\n",
      "train loss:0.1439400507074337\n",
      "train loss:0.03768590959453484\n",
      "train loss:0.14339702053867273\n",
      "train loss:0.0076602920521384186\n",
      "train loss:0.07251332448983909\n",
      "train loss:0.029632376735217993\n",
      "train loss:0.021229352930543864\n",
      "train loss:0.024508434573804402\n",
      "train loss:0.013269624220892548\n",
      "train loss:0.012835102380321818\n",
      "train loss:0.02418879447169526\n",
      "train loss:0.019859540101154136\n",
      "train loss:0.08671358972929678\n",
      "train loss:0.09999321341655146\n",
      "train loss:0.0514009952231171\n",
      "train loss:0.03664329253373985\n",
      "train loss:0.011397547323260124\n",
      "train loss:0.0616636900361453\n",
      "train loss:0.01494685215552311\n",
      "train loss:0.009017245188363889\n",
      "train loss:0.019879488918243413\n",
      "train loss:0.03611426737716021\n",
      "train loss:0.05901776359927988\n",
      "train loss:0.01822069228684544\n",
      "train loss:0.04271487717821709\n",
      "train loss:0.040809239587370365\n",
      "train loss:0.036447746593395204\n",
      "train loss:0.012360843845077985\n",
      "train loss:0.01312312064909156\n",
      "train loss:0.016764742674150096\n",
      "train loss:0.02557521529060658\n",
      "train loss:0.01230932116161745\n",
      "train loss:0.064077763275162\n",
      "train loss:0.032264569458495644\n",
      "train loss:0.02675642328251694\n",
      "train loss:0.009973532559950093\n",
      "train loss:0.04488622155534747\n",
      "train loss:0.01726509958411446\n",
      "train loss:0.01856383241093542\n",
      "train loss:0.051115979051142874\n",
      "train loss:0.00864687677603392\n",
      "train loss:0.046482226307990065\n",
      "train loss:0.08228966396391188\n",
      "train loss:0.07073815705872782\n",
      "train loss:0.019831207820014102\n",
      "train loss:0.019251761293058892\n",
      "train loss:0.047253908080267\n",
      "train loss:0.04341502679399402\n",
      "train loss:0.08461914043547276\n",
      "train loss:0.033812134875813575\n",
      "train loss:0.026036350002711487\n",
      "train loss:0.10123531448354729\n",
      "train loss:0.045468540943027304\n",
      "train loss:0.005563208158104119\n",
      "train loss:0.016124672901625777\n",
      "train loss:0.034377173507638276\n",
      "train loss:0.03041950921054767\n",
      "train loss:0.0047271554408571045\n",
      "train loss:0.025479894110058444\n",
      "train loss:0.017970618483114095\n",
      "train loss:0.030456690102482815\n",
      "train loss:0.04167907530849194\n",
      "train loss:0.03736451276000477\n",
      "train loss:0.018869368556116255\n",
      "train loss:0.02408017873217429\n",
      "train loss:0.015573938390904384\n",
      "train loss:0.022216214016087338\n",
      "train loss:0.00983749552487351\n",
      "train loss:0.0507947110745067\n",
      "train loss:0.012324377488239653\n",
      "train loss:0.011855479759023856\n",
      "train loss:0.013343155216619327\n",
      "train loss:0.003976168945649411\n",
      "train loss:0.01530404153454364\n",
      "train loss:0.019741068447741556\n",
      "train loss:0.0397983214319464\n",
      "train loss:0.01852263044630314\n",
      "train loss:0.020815529306791064\n",
      "train loss:0.016037245713610808\n",
      "train loss:0.02343725291910381\n",
      "train loss:0.0066456427196543225\n",
      "train loss:0.023616576626594218\n",
      "train loss:0.07541406208206973\n",
      "=== epoch:5, train acc:0.988, test acc:0.985 ===\n",
      "train loss:0.05189106216610397\n",
      "train loss:0.033993337957618074\n",
      "train loss:0.018047550070496708\n",
      "train loss:0.023712882366079668\n",
      "train loss:0.043164863213147615\n",
      "train loss:0.029362803213565495\n",
      "train loss:0.011597531933042991\n",
      "train loss:0.054492651050434826\n",
      "train loss:0.018386242130577723\n",
      "train loss:0.008588801673845743\n",
      "train loss:0.05571128184841757\n",
      "train loss:0.003823295604742199\n",
      "train loss:0.04385945989964793\n",
      "train loss:0.03877534673902484\n",
      "train loss:0.044411916043538405\n",
      "train loss:0.007923734612783723\n",
      "train loss:0.04802262243865422\n",
      "train loss:0.01610877693277479\n",
      "train loss:0.014521105176050583\n",
      "train loss:0.049506351188092976\n",
      "train loss:0.039044946122671564\n",
      "train loss:0.03534291984201949\n",
      "train loss:0.040220236205832725\n",
      "train loss:0.024134271344301746\n",
      "train loss:0.031341735909211174\n",
      "train loss:0.019946648932390117\n",
      "train loss:0.020444016316899807\n",
      "train loss:0.02582024048246091\n",
      "train loss:0.016274602152528\n",
      "train loss:0.0062203674604663\n",
      "train loss:0.03614847719524322\n",
      "train loss:0.021144052971257602\n",
      "train loss:0.011104342152327666\n",
      "train loss:0.0028096040346605004\n",
      "train loss:0.032352604505168574\n",
      "train loss:0.009937978673372703\n",
      "train loss:0.007833865384295556\n",
      "train loss:0.009067430968927237\n",
      "train loss:0.016055135637367445\n",
      "train loss:0.02406661240059719\n",
      "train loss:0.03073350352632814\n",
      "train loss:0.03889494779429612\n",
      "train loss:0.01880596173846839\n",
      "train loss:0.012098391057397883\n",
      "train loss:0.014653733372720778\n",
      "train loss:0.012931789087346397\n",
      "train loss:0.04130450406105164\n",
      "train loss:0.014486588313534039\n",
      "train loss:0.08889058463522083\n",
      "train loss:0.03140223850431085\n",
      "train loss:0.08091151464360552\n",
      "train loss:0.016152215274112872\n",
      "train loss:0.030783211080832283\n",
      "train loss:0.014782220393942053\n",
      "train loss:0.009526707425847578\n",
      "train loss:0.03539535267161719\n",
      "train loss:0.02412682051425741\n",
      "train loss:0.016585089283235786\n",
      "train loss:0.00979892155900457\n",
      "train loss:0.01790746822831977\n",
      "train loss:0.0332750748533497\n",
      "train loss:0.05470352758632768\n",
      "train loss:0.0458568235085103\n",
      "train loss:0.024597624670303134\n",
      "train loss:0.037945005583304256\n",
      "train loss:0.015938577182217514\n",
      "train loss:0.0112003652560171\n",
      "train loss:0.09063241490034256\n",
      "train loss:0.020528480079932312\n",
      "train loss:0.014981168809401448\n",
      "train loss:0.07266901531717586\n",
      "train loss:0.023070455331873366\n",
      "train loss:0.0220064712747396\n",
      "train loss:0.0058185444294248\n",
      "train loss:0.011353476720568524\n",
      "train loss:0.014233894260603255\n",
      "train loss:0.010099592256646313\n",
      "train loss:0.05260086752628608\n",
      "train loss:0.013572544441607931\n",
      "train loss:0.009780255024476868\n",
      "train loss:0.013198964106432226\n",
      "train loss:0.08039542591932952\n",
      "train loss:0.06986071501623115\n",
      "train loss:0.01506036158650619\n",
      "train loss:0.007835510708431415\n",
      "train loss:0.01831831326253732\n",
      "train loss:0.021664385107928202\n",
      "train loss:0.01757178222339655\n",
      "train loss:0.021057911661715934\n",
      "train loss:0.05654107320757165\n",
      "train loss:0.013845463515109575\n",
      "train loss:0.028547682923128977\n",
      "train loss:0.014177471967298055\n",
      "train loss:0.011414383214090188\n",
      "train loss:0.013157240977349159\n",
      "train loss:0.020299334064398225\n",
      "train loss:0.06992873709507483\n",
      "train loss:0.17906660469357427\n",
      "train loss:0.0166726288149312\n",
      "train loss:0.02572717288240732\n",
      "train loss:0.004283437399301606\n",
      "train loss:0.023492382971068374\n",
      "train loss:0.05333867499868378\n",
      "train loss:0.06045173722288324\n",
      "train loss:0.049725090493853194\n",
      "train loss:0.038420427910904276\n",
      "train loss:0.005459235035461842\n",
      "train loss:0.0628604971791506\n",
      "train loss:0.014494942084412397\n",
      "train loss:0.04400609635479494\n",
      "train loss:0.02809294112077297\n",
      "train loss:0.010742494742657524\n",
      "train loss:0.055357971214767196\n",
      "train loss:0.06158496454149665\n",
      "train loss:0.015011168254015423\n",
      "train loss:0.06071098282310496\n",
      "train loss:0.12102600872226671\n",
      "train loss:0.013327551807792071\n",
      "train loss:0.03932146414438568\n",
      "train loss:0.05378305075551589\n",
      "train loss:0.04614036504119869\n",
      "train loss:0.019560552504493154\n",
      "train loss:0.012872422702946378\n",
      "train loss:0.03761566078672014\n",
      "train loss:0.03200010595822829\n",
      "train loss:0.018426865085548777\n",
      "train loss:0.052954501909739396\n",
      "train loss:0.00665340319334274\n",
      "train loss:0.10981332787457256\n",
      "train loss:0.061857060574944514\n",
      "train loss:0.03821794440405145\n",
      "train loss:0.03228750169860741\n",
      "train loss:0.015343749446665325\n",
      "train loss:0.0414778232050846\n",
      "train loss:0.023947042722296003\n",
      "train loss:0.033213042389310374\n",
      "train loss:0.018285306188950728\n",
      "train loss:0.016951590908633428\n",
      "train loss:0.018087722796337226\n",
      "train loss:0.031021094228352128\n",
      "train loss:0.0367546150875539\n",
      "train loss:0.0445905380582969\n",
      "train loss:0.049540874381508795\n",
      "train loss:0.011961251313102484\n",
      "train loss:0.054993131245842604\n",
      "train loss:0.09678995629081248\n",
      "train loss:0.009001260205432666\n",
      "train loss:0.021483506674315825\n",
      "train loss:0.03540540324505596\n",
      "train loss:0.018695892089320855\n",
      "train loss:0.008038666198975732\n",
      "train loss:0.051406738201453264\n",
      "train loss:0.032296228254713275\n",
      "train loss:0.08958345205328534\n",
      "train loss:0.06850854118126383\n",
      "train loss:0.015265636435734965\n",
      "train loss:0.05449190428458489\n",
      "train loss:0.05402618468740175\n",
      "train loss:0.0252475289717166\n",
      "train loss:0.03565563643114106\n",
      "train loss:0.022338777710948342\n",
      "train loss:0.008596300570341558\n",
      "train loss:0.015595334608564403\n",
      "train loss:0.07102673758095962\n",
      "train loss:0.014369623683193807\n",
      "train loss:0.011536273479345494\n",
      "train loss:0.035899314545797284\n",
      "train loss:0.012325689555073634\n",
      "train loss:0.014717060171227076\n",
      "train loss:0.0154286307582732\n",
      "train loss:0.053400527223352444\n",
      "train loss:0.09207771407061997\n",
      "train loss:0.053376316069373306\n",
      "train loss:0.021656390496769203\n",
      "train loss:0.011057678912008224\n",
      "train loss:0.01748239655113941\n",
      "train loss:0.03672187108956809\n",
      "train loss:0.01733785293796857\n",
      "train loss:0.04632925823678471\n",
      "train loss:0.014217135115133664\n",
      "train loss:0.010790644038477858\n",
      "train loss:0.044522594644400844\n",
      "train loss:0.020418415179416796\n",
      "train loss:0.018551597586385776\n",
      "train loss:0.02788961794051724\n",
      "train loss:0.016196717448611717\n",
      "train loss:0.05583054549272135\n",
      "train loss:0.014769704011629914\n",
      "train loss:0.03014638910923362\n",
      "train loss:0.0421689120758763\n",
      "train loss:0.034463428674332844\n",
      "train loss:0.010207944646496414\n",
      "train loss:0.051551187527541745\n",
      "train loss:0.05791687510601985\n",
      "train loss:0.014986602734699768\n",
      "train loss:0.02150632555783459\n",
      "train loss:0.05512598391526322\n",
      "train loss:0.0046672962822032074\n",
      "train loss:0.04204626212653054\n",
      "train loss:0.06665794125115135\n",
      "train loss:0.08182124667284199\n",
      "train loss:0.03854957783667057\n",
      "train loss:0.022955991452671843\n",
      "train loss:0.014374288126937789\n",
      "train loss:0.009937557802257378\n",
      "train loss:0.0026651810894354065\n",
      "train loss:0.009042225847985563\n",
      "train loss:0.06856931427508459\n",
      "train loss:0.017644799126373886\n",
      "train loss:0.014154274158762864\n",
      "train loss:0.012187471386339797\n",
      "train loss:0.07790879187353546\n",
      "train loss:0.013182886362450122\n",
      "train loss:0.02351452038186961\n",
      "train loss:0.012124185352995039\n",
      "train loss:0.03318587215198715\n",
      "train loss:0.09404113001315849\n",
      "train loss:0.01806870550660634\n",
      "train loss:0.02281988774746386\n",
      "train loss:0.051692815590915\n",
      "train loss:0.04024028054678973\n",
      "train loss:0.016361648587976948\n",
      "train loss:0.038755264207650526\n",
      "train loss:0.02247714772158043\n",
      "train loss:0.025991674632009486\n",
      "train loss:0.01022209462813298\n",
      "train loss:0.032303684106275196\n",
      "train loss:0.04883255551578891\n",
      "train loss:0.01047139846931168\n",
      "train loss:0.017908160646443808\n",
      "train loss:0.027313038003369962\n",
      "train loss:0.022646972002390728\n",
      "train loss:0.0065098390652458815\n",
      "train loss:0.01683035482213277\n",
      "train loss:0.09517805800372361\n",
      "train loss:0.11130324273993314\n",
      "train loss:0.02527526478039458\n",
      "train loss:0.061519323461021694\n",
      "train loss:0.030947846479640207\n",
      "train loss:0.053229253289467565\n",
      "train loss:0.017508726030907994\n",
      "train loss:0.03784659106902332\n",
      "train loss:0.04725997640565358\n",
      "train loss:0.017207899549403452\n",
      "train loss:0.019949667168848315\n",
      "train loss:0.005536984451760774\n",
      "train loss:0.05484989159707377\n",
      "train loss:0.017416472706678187\n",
      "train loss:0.02319092851655009\n",
      "train loss:0.018947880695626078\n",
      "train loss:0.02454957817241947\n",
      "train loss:0.026321707658122293\n",
      "train loss:0.07627117788384984\n",
      "train loss:0.04857104613942396\n",
      "train loss:0.03405802664918243\n",
      "train loss:0.005353886445883677\n",
      "train loss:0.07470079960579738\n",
      "train loss:0.04439231785427149\n",
      "train loss:0.012433578813701924\n",
      "train loss:0.026188279932797588\n",
      "train loss:0.008022594406212566\n",
      "train loss:0.00889113585755659\n",
      "train loss:0.018416889253354118\n",
      "train loss:0.013759962011383284\n",
      "train loss:0.016540901324674343\n",
      "train loss:0.025272909116089436\n",
      "train loss:0.036718590893980704\n",
      "train loss:0.019410490329601744\n",
      "train loss:0.026929044527984294\n",
      "train loss:0.021968853840592502\n",
      "train loss:0.09432354000724992\n",
      "train loss:0.04808490606798525\n",
      "train loss:0.014107541733395665\n",
      "train loss:0.004863681745525299\n",
      "train loss:0.01705898021214305\n",
      "train loss:0.050986472720772724\n",
      "train loss:0.01773385538505484\n",
      "train loss:0.11773402473406593\n",
      "train loss:0.01929331207202924\n",
      "train loss:0.01005297204383545\n",
      "train loss:0.017586626452245784\n",
      "train loss:0.011731901766941255\n",
      "train loss:0.03787986912176424\n",
      "train loss:0.04018404400755407\n",
      "train loss:0.00627573899348021\n",
      "train loss:0.0483071700235341\n",
      "train loss:0.024749460922208918\n",
      "train loss:0.010306846259594049\n",
      "train loss:0.06009920050532468\n",
      "train loss:0.016121223048711414\n",
      "train loss:0.008633604128701053\n",
      "train loss:0.012212401477129826\n",
      "train loss:0.013728504561836878\n",
      "train loss:0.024697681523112164\n",
      "train loss:0.03364649314113168\n",
      "train loss:0.01867117336733888\n",
      "train loss:0.012150865093555506\n",
      "train loss:0.04025169624900522\n",
      "train loss:0.005999062448936474\n",
      "train loss:0.051183375950079865\n",
      "train loss:0.0069251418285206446\n",
      "train loss:0.03260173383382578\n",
      "train loss:0.02325674402969632\n",
      "train loss:0.02379791527464691\n",
      "train loss:0.011158042728700857\n",
      "train loss:0.03393116930991242\n",
      "train loss:0.007109046871875342\n",
      "train loss:0.029656446500251094\n",
      "train loss:0.023257103789432354\n",
      "train loss:0.10417830014181606\n",
      "train loss:0.057041884307279925\n",
      "train loss:0.026733166463662958\n",
      "train loss:0.025620126082831374\n",
      "train loss:0.05997393611904821\n",
      "train loss:0.032876388131353844\n",
      "train loss:0.04137755873019196\n",
      "train loss:0.018155332955433486\n",
      "train loss:0.13143330172360107\n",
      "train loss:0.021260997610951664\n",
      "train loss:0.0558358643949155\n",
      "train loss:0.025291458332325198\n",
      "train loss:0.031569295497015\n",
      "train loss:0.029406699846213757\n",
      "train loss:0.10402279142842195\n",
      "train loss:0.017471077983315823\n",
      "train loss:0.014708704283911007\n",
      "train loss:0.0766142298313011\n",
      "train loss:0.01383525082931069\n",
      "train loss:0.04120916169808662\n",
      "train loss:0.028282575118846225\n",
      "train loss:0.03447657751097093\n",
      "train loss:0.024601243718693672\n",
      "train loss:0.022461552285148043\n",
      "train loss:0.04546832240778016\n",
      "train loss:0.022709808859930045\n",
      "train loss:0.014487914819170269\n",
      "train loss:0.02288552232923514\n",
      "train loss:0.019210040750164227\n",
      "train loss:0.019450050544842\n",
      "train loss:0.03174818817810446\n",
      "train loss:0.03460279147441583\n",
      "train loss:0.014597055763275605\n",
      "train loss:0.018922885869530746\n",
      "train loss:0.02646189585784889\n",
      "train loss:0.005948128098425233\n",
      "train loss:0.005775094044862943\n",
      "train loss:0.015172044906584468\n",
      "train loss:0.017867880445014382\n",
      "train loss:0.02166569475973112\n",
      "train loss:0.010283292232089463\n",
      "train loss:0.011762041146808708\n",
      "train loss:0.048393995371063855\n",
      "train loss:0.038149539103888096\n",
      "train loss:0.05176226994298845\n",
      "train loss:0.04882027626492682\n",
      "train loss:0.05552223860770487\n",
      "train loss:0.07172521045896435\n",
      "train loss:0.05641166554371923\n",
      "train loss:0.016665174682626092\n",
      "train loss:0.028557176268699417\n",
      "train loss:0.009707392773890537\n",
      "train loss:0.02372815027830872\n",
      "train loss:0.008423219640331176\n",
      "train loss:0.16729810443005189\n",
      "train loss:0.028448836210653635\n",
      "train loss:0.03343776959445552\n",
      "train loss:0.015227447454136749\n",
      "train loss:0.053859857911294995\n",
      "train loss:0.0491095876605351\n",
      "train loss:0.0036938076461366263\n",
      "train loss:0.11140009523758047\n",
      "train loss:0.006237853366438905\n",
      "train loss:0.03138153423247027\n",
      "train loss:0.04067933766283163\n",
      "train loss:0.006203593624327486\n",
      "train loss:0.024519199280604868\n",
      "train loss:0.036482644166520736\n",
      "train loss:0.02059090610389811\n",
      "train loss:0.02572073744132224\n",
      "train loss:0.005453481126070905\n",
      "train loss:0.039462568671584555\n",
      "train loss:0.013350893758774747\n",
      "train loss:0.049285314271873194\n",
      "train loss:0.02606864413685131\n",
      "train loss:0.04497223550723096\n",
      "train loss:0.012666663836425316\n",
      "train loss:0.04235968863263632\n",
      "train loss:0.028220122710668635\n",
      "train loss:0.04149339752086942\n",
      "train loss:0.014093481289621863\n",
      "train loss:0.015988262123407405\n",
      "train loss:0.030521207568665415\n",
      "train loss:0.03362894344193632\n",
      "train loss:0.006227881965931513\n",
      "train loss:0.028353103145000013\n",
      "train loss:0.01745159042032545\n",
      "train loss:0.02066178619656253\n",
      "train loss:0.023099937291308464\n",
      "train loss:0.046590917501999736\n",
      "train loss:0.019649724477042982\n",
      "train loss:0.016072319174114575\n",
      "train loss:0.009455309002623133\n",
      "train loss:0.005354229280181518\n",
      "train loss:0.0146593444380679\n",
      "train loss:0.018359005268463513\n",
      "train loss:0.006740008852884668\n",
      "train loss:0.06108762743430937\n",
      "train loss:0.016708726391649643\n",
      "train loss:0.04373863491485918\n",
      "train loss:0.02775451394810926\n",
      "train loss:0.01322017697327951\n",
      "train loss:0.004776268439790487\n",
      "train loss:0.011031034618572688\n",
      "train loss:0.0484706161673385\n",
      "train loss:0.0076206487489210864\n",
      "train loss:0.01612099284132063\n",
      "train loss:0.024144310103726592\n",
      "train loss:0.011484614559643338\n",
      "train loss:0.015371665083340882\n",
      "train loss:0.011797082648340425\n",
      "train loss:0.021444380912291675\n",
      "train loss:0.01693114498229776\n",
      "train loss:0.04013491109626291\n",
      "train loss:0.04134258810189494\n",
      "train loss:0.015182933201155196\n",
      "train loss:0.008606784768754525\n",
      "train loss:0.03812904768308791\n",
      "train loss:0.027618983474174925\n",
      "train loss:0.02868999098291861\n",
      "train loss:0.003151922119271985\n",
      "train loss:0.006442691586638485\n",
      "train loss:0.006861498314377833\n",
      "train loss:0.010860040014925547\n",
      "train loss:0.007106661138219758\n",
      "train loss:0.10336753123499041\n",
      "train loss:0.027754060915944304\n",
      "train loss:0.029092819706396034\n",
      "train loss:0.00975588864604117\n",
      "train loss:0.01876969418229336\n",
      "train loss:0.042795951595087194\n",
      "train loss:0.005671967305258607\n",
      "train loss:0.014686719863201508\n",
      "train loss:0.003190355605619124\n",
      "train loss:0.003172830232465083\n",
      "train loss:0.01759824588031776\n",
      "train loss:0.005854833350228696\n",
      "train loss:0.02118883122614666\n",
      "train loss:0.08358614177941621\n",
      "train loss:0.010225411209623914\n",
      "train loss:0.007571309995831042\n",
      "train loss:0.0029568344757660396\n",
      "train loss:0.018307392639485387\n",
      "train loss:0.06752416715046905\n",
      "train loss:0.01333815549479573\n",
      "train loss:0.00744999034088458\n",
      "train loss:0.01264008154264457\n",
      "train loss:0.026552165507106183\n",
      "train loss:0.011544754773176681\n",
      "train loss:0.0076158238048680904\n",
      "train loss:0.02771602850752476\n",
      "train loss:0.0018850380980482253\n",
      "train loss:0.021663216024117582\n",
      "train loss:0.03190614896476049\n",
      "train loss:0.017952929201453936\n",
      "train loss:0.03209142495833682\n",
      "train loss:0.01727457177406597\n",
      "train loss:0.017099215127656518\n",
      "train loss:0.03769728837094385\n",
      "train loss:0.09466618465668139\n",
      "train loss:0.009584264626177449\n",
      "train loss:0.015725789018010902\n",
      "train loss:0.010951047354761354\n",
      "train loss:0.018781990763872235\n",
      "train loss:0.024464437707480653\n",
      "train loss:0.009656826458288912\n",
      "train loss:0.055918551845576314\n",
      "train loss:0.021679813963084806\n",
      "train loss:0.047824702717765594\n",
      "train loss:0.02059865134858253\n",
      "train loss:0.015979764045702904\n",
      "train loss:0.018778795282364907\n",
      "train loss:0.03460292866326731\n",
      "train loss:0.028088010290543416\n",
      "train loss:0.04192880253092361\n",
      "train loss:0.04228073539407756\n",
      "train loss:0.047084619415113806\n",
      "train loss:0.01874117262225125\n",
      "train loss:0.009891864596985937\n",
      "train loss:0.04387993926430723\n",
      "train loss:0.014858329837130478\n",
      "train loss:0.09624411815754401\n",
      "train loss:0.0876041585769043\n",
      "train loss:0.13033810397976076\n",
      "train loss:0.027814607720105952\n",
      "train loss:0.0331116823773678\n",
      "train loss:0.015206233896622783\n",
      "train loss:0.019408486540174134\n",
      "train loss:0.00835311296915299\n",
      "train loss:0.032318342121224176\n",
      "train loss:0.04215729303586396\n",
      "train loss:0.01001104040500616\n",
      "train loss:0.013704761422065464\n",
      "train loss:0.01665497459088551\n",
      "train loss:0.02161083116983647\n",
      "train loss:0.02295054014576731\n",
      "train loss:0.08322705785059745\n",
      "train loss:0.025357469993280268\n",
      "train loss:0.005705090461929939\n",
      "train loss:0.10244385501256488\n",
      "train loss:0.01739500749173707\n",
      "train loss:0.013316942064495942\n",
      "train loss:0.07101578786890422\n",
      "train loss:0.017692740495974683\n",
      "train loss:0.019890309272247855\n",
      "train loss:0.03884794742662876\n",
      "train loss:0.010289220190930447\n",
      "train loss:0.09768605148345241\n",
      "train loss:0.02347251013437374\n",
      "train loss:0.01856825758451412\n",
      "train loss:0.011165102279922935\n",
      "train loss:0.050881746157108186\n",
      "train loss:0.02294701089933514\n",
      "train loss:0.018622421405684902\n",
      "train loss:0.01885273643751646\n",
      "train loss:0.07821383846115744\n",
      "train loss:0.03838752745135141\n",
      "train loss:0.0199727313203538\n",
      "train loss:0.025076191096545394\n",
      "train loss:0.003946539601918857\n",
      "train loss:0.050903341363843745\n",
      "train loss:0.03741040804421633\n",
      "train loss:0.028962816080351318\n",
      "train loss:0.01262216816873902\n",
      "train loss:0.01848879025363001\n",
      "train loss:0.009009373981393794\n",
      "train loss:0.04367497406077769\n",
      "train loss:0.03656519124839012\n",
      "train loss:0.028771348326755573\n",
      "train loss:0.014853674815288338\n",
      "train loss:0.004374639769505324\n",
      "train loss:0.03536305129322388\n",
      "train loss:0.019185092391168786\n",
      "train loss:0.01298853419246222\n",
      "train loss:0.05202709494009567\n",
      "train loss:0.011363618697603379\n",
      "train loss:0.008769467138393034\n",
      "train loss:0.017397976605783298\n",
      "train loss:0.034933917324650905\n",
      "train loss:0.10310683669019237\n",
      "train loss:0.02534270025008029\n",
      "train loss:0.014193724466823859\n",
      "train loss:0.00611954752569455\n",
      "train loss:0.00902373247664922\n",
      "train loss:0.08894761893969497\n",
      "train loss:0.01214238039871898\n",
      "train loss:0.03563996087758146\n",
      "train loss:0.010461920232452238\n",
      "train loss:0.04641779235659251\n",
      "train loss:0.03805977302249041\n",
      "train loss:0.06802534881997362\n",
      "train loss:0.002904946173174815\n",
      "train loss:0.020290025825731167\n",
      "train loss:0.012462239920234099\n",
      "train loss:0.05405573980308069\n",
      "train loss:0.01502733684902379\n",
      "train loss:0.03491732246962777\n",
      "train loss:0.02447005824569902\n",
      "train loss:0.026131880902363867\n",
      "train loss:0.10462803857381338\n",
      "train loss:0.01865229007071114\n",
      "train loss:0.016432399721119408\n",
      "train loss:0.017966086127300773\n",
      "train loss:0.015361014701524803\n",
      "train loss:0.12212672723556775\n",
      "train loss:0.05432523573480395\n",
      "train loss:0.01708385195308041\n",
      "train loss:0.04806972747151397\n",
      "train loss:0.030602074376373616\n",
      "train loss:0.01090359205337571\n",
      "train loss:0.026128209395738514\n",
      "train loss:0.01949301835379294\n",
      "train loss:0.048010460080524356\n",
      "train loss:0.010646532975219336\n",
      "train loss:0.028206150617924374\n",
      "train loss:0.02614875739421431\n",
      "train loss:0.023983114263812163\n",
      "train loss:0.009058916624149961\n",
      "train loss:0.01533453188951999\n",
      "train loss:0.01262431609925232\n",
      "train loss:0.04226298077547405\n",
      "train loss:0.030143581904826952\n",
      "train loss:0.0313309110369861\n",
      "train loss:0.019019011081483735\n",
      "train loss:0.012541276550038059\n",
      "train loss:0.013448651390168473\n",
      "train loss:0.08762959600656811\n",
      "train loss:0.02008561691629905\n",
      "train loss:0.01036616615705207\n",
      "train loss:0.0138657238636797\n",
      "train loss:0.08317321594846072\n",
      "=== epoch:6, train acc:0.989, test acc:0.985 ===\n",
      "train loss:0.01889757096388363\n",
      "train loss:0.04221963821080675\n",
      "train loss:0.028879152654917792\n",
      "train loss:0.0073829730807095754\n",
      "train loss:0.059089996693662446\n",
      "train loss:0.018852450281806654\n",
      "train loss:0.006340208240701437\n",
      "train loss:0.009336241096994024\n",
      "train loss:0.05461758173041016\n",
      "train loss:0.006584525662060803\n",
      "train loss:0.06454083274623248\n",
      "train loss:0.02364086862985047\n",
      "train loss:0.018472659537005012\n",
      "train loss:0.007795761395923523\n",
      "train loss:0.0279335312187517\n",
      "train loss:0.00517387021700783\n",
      "train loss:0.018496326367545425\n",
      "train loss:0.010904215957594585\n",
      "train loss:0.036273370434279514\n",
      "train loss:0.09362210599236091\n",
      "train loss:0.06710146700219605\n",
      "train loss:0.04173026944720127\n",
      "train loss:0.020302960097681753\n",
      "train loss:0.04294029102367256\n",
      "train loss:0.009679686795373128\n",
      "train loss:0.03753328021458233\n",
      "train loss:0.01012797520296967\n",
      "train loss:0.03528264619522954\n",
      "train loss:0.03090436420039292\n",
      "train loss:0.005727071826915801\n",
      "train loss:0.014331818107055727\n",
      "train loss:0.009060053937551571\n",
      "train loss:0.011314448907303765\n",
      "train loss:0.033568865017742214\n",
      "train loss:0.019596606966389863\n",
      "train loss:0.015809074885327613\n",
      "train loss:0.012787839108788147\n",
      "train loss:0.011958816881497\n",
      "train loss:0.011244155360379674\n",
      "train loss:0.048098079837299614\n",
      "train loss:0.019118915963207493\n",
      "train loss:0.025068418140661707\n",
      "train loss:0.017355585174654183\n",
      "train loss:0.017366954596435395\n",
      "train loss:0.19123723752381672\n",
      "train loss:0.00927673296958479\n",
      "train loss:0.02325298268091824\n",
      "train loss:0.016971681574372812\n",
      "train loss:0.061684387078139086\n",
      "train loss:0.06501678177814126\n",
      "train loss:0.02612034001201926\n",
      "train loss:0.03503820494442789\n",
      "train loss:0.006170489544685659\n",
      "train loss:0.00861097106638741\n",
      "train loss:0.04338289731651661\n",
      "train loss:0.012027519062020604\n",
      "train loss:0.024743020462775888\n",
      "train loss:0.025066014778556068\n",
      "train loss:0.015986672197012816\n",
      "train loss:0.00440217965032585\n",
      "train loss:0.006085675891340451\n",
      "train loss:0.02033048234771463\n",
      "train loss:0.01744983455606974\n",
      "train loss:0.08434669709849581\n",
      "train loss:0.02256377383287061\n",
      "train loss:0.019743500299151104\n",
      "train loss:0.029516419977939235\n",
      "train loss:0.02528567121109288\n",
      "train loss:0.01832281435379136\n",
      "train loss:0.020705005776274925\n",
      "train loss:0.015157218411137334\n",
      "train loss:0.005968746348121241\n",
      "train loss:0.007335362834572311\n",
      "train loss:0.033741350968658476\n",
      "train loss:0.00789865928424722\n",
      "train loss:0.018355316052483125\n",
      "train loss:0.04846964216096125\n",
      "train loss:0.02582676309741583\n",
      "train loss:0.02894384986471172\n",
      "train loss:0.023780688511862377\n",
      "train loss:0.11281607745408072\n",
      "train loss:0.0057403350369251\n",
      "train loss:0.0171928778563089\n",
      "train loss:0.01079583494071253\n",
      "train loss:0.003530038590047223\n",
      "train loss:0.009683218649289317\n",
      "train loss:0.015390001731680474\n",
      "train loss:0.008686685159195169\n",
      "train loss:0.014333654275240693\n",
      "train loss:0.020231560125757938\n",
      "train loss:0.007491130382311183\n",
      "train loss:0.01486697882681089\n",
      "train loss:0.005991017449737184\n",
      "train loss:0.026992832005506805\n",
      "train loss:0.08154539089901004\n",
      "train loss:0.02060424857000576\n",
      "train loss:0.008193761009067891\n",
      "train loss:0.025503925321136878\n",
      "train loss:0.01163524050071898\n",
      "train loss:0.03038329451132745\n",
      "train loss:0.008898983521994638\n",
      "train loss:0.022669204709091985\n",
      "train loss:0.01970337193959163\n",
      "train loss:0.03828600634344806\n",
      "train loss:0.006860841242005867\n",
      "train loss:0.00763864517933722\n",
      "train loss:0.005505266652170418\n",
      "train loss:0.02830645667286091\n",
      "train loss:0.008698022915046752\n",
      "train loss:0.005529898040482111\n",
      "train loss:0.009924453301056894\n",
      "train loss:0.029915482593847256\n",
      "train loss:0.034850021512414626\n",
      "train loss:0.014634701820228326\n",
      "train loss:0.021711603954163015\n",
      "train loss:0.009837652216941696\n",
      "train loss:0.06413859693623628\n",
      "train loss:0.0304084726012995\n",
      "train loss:0.007834830372532763\n",
      "train loss:0.029964702560451684\n",
      "train loss:0.013422291135539756\n",
      "train loss:0.01228801208088227\n",
      "train loss:0.012199901132074382\n",
      "train loss:0.07538717383241778\n",
      "train loss:0.029152933874469298\n",
      "train loss:0.0072181208474606075\n",
      "train loss:0.023630320302236302\n",
      "train loss:0.00897437452281113\n",
      "train loss:0.030613070839724688\n",
      "train loss:0.01471048472007722\n",
      "train loss:0.019265303861414982\n",
      "train loss:0.012241657607515648\n",
      "train loss:0.006724796714775539\n",
      "train loss:0.06439673903071384\n",
      "train loss:0.0022172183501948713\n",
      "train loss:0.026277709274719157\n",
      "train loss:0.0021931506684648005\n",
      "train loss:0.017902616313660902\n",
      "train loss:0.007085047063299772\n",
      "train loss:0.053225280666395286\n",
      "train loss:0.022896210625718814\n",
      "train loss:0.01427748239280483\n",
      "train loss:0.008455768130302832\n",
      "train loss:0.021482638810941986\n",
      "train loss:0.04152702290361592\n",
      "train loss:0.10741455705957574\n",
      "train loss:0.022452351158620945\n",
      "train loss:0.02590715412769031\n",
      "train loss:0.005272542923909211\n",
      "train loss:0.030088446042334346\n",
      "train loss:0.0201956482984807\n",
      "train loss:0.013198853913824405\n",
      "train loss:0.00668947507330387\n",
      "train loss:0.0040896369409070075\n",
      "train loss:0.02081838440536319\n",
      "train loss:0.033822885010720796\n",
      "train loss:0.023688697476240785\n",
      "train loss:0.04523974416466655\n",
      "train loss:0.06104916284249766\n",
      "train loss:0.004694064959632656\n",
      "train loss:0.043372617055375165\n",
      "train loss:0.02037692430164955\n",
      "train loss:0.07332603103652047\n",
      "train loss:0.008858712551074465\n",
      "train loss:0.009448136664332564\n",
      "train loss:0.012388315004715902\n",
      "train loss:0.005408805256521728\n",
      "train loss:0.003903785425775767\n",
      "train loss:0.023425764318575192\n",
      "train loss:0.026664678350567586\n",
      "train loss:0.029634522247239007\n",
      "train loss:0.00882776736449095\n",
      "train loss:0.024514028073181714\n",
      "train loss:0.021300499150310036\n",
      "train loss:0.052780747997106144\n",
      "train loss:0.051455897457977476\n",
      "train loss:0.016317482434038515\n",
      "train loss:0.06835553728456147\n",
      "train loss:0.019603001046851896\n",
      "train loss:0.028567727250861916\n",
      "train loss:0.022281563053748545\n",
      "train loss:0.03859287267620715\n",
      "train loss:0.009719760442108072\n",
      "train loss:0.019562553059952654\n",
      "train loss:0.055880423326390226\n",
      "train loss:0.0031258707429641036\n",
      "train loss:0.01569266214891849\n",
      "train loss:0.026085544939339587\n",
      "train loss:0.008812480117819226\n",
      "train loss:0.012071027198630893\n",
      "train loss:0.011440489015842093\n",
      "train loss:0.04331086454677505\n",
      "train loss:0.004818134809300006\n",
      "train loss:0.015840857343823175\n",
      "train loss:0.050997235665364335\n",
      "train loss:0.027258828344483045\n",
      "train loss:0.014814674040305802\n",
      "train loss:0.012980547486302547\n",
      "train loss:0.021269836496150586\n",
      "train loss:0.035934490002269094\n",
      "train loss:0.0038192045746406294\n",
      "train loss:0.01153640076700247\n",
      "train loss:0.012226089404309876\n",
      "train loss:0.06446501337853426\n",
      "train loss:0.04101517152354644\n",
      "train loss:0.005986771881534191\n",
      "train loss:0.015719030384017472\n",
      "train loss:0.0007827989007626421\n",
      "train loss:0.017931045030149285\n",
      "train loss:0.04787465960856449\n",
      "train loss:0.016058385148542728\n",
      "train loss:0.010383188964084734\n",
      "train loss:0.026914155471860542\n",
      "train loss:0.048384424604633845\n",
      "train loss:0.031048162514272226\n",
      "train loss:0.017027780285431858\n",
      "train loss:0.006440332162426399\n",
      "train loss:0.009683929543749888\n",
      "train loss:0.010000283163908996\n",
      "train loss:0.03210081609653606\n",
      "train loss:0.0017015506340888513\n",
      "train loss:0.01011144030021441\n",
      "train loss:0.029695318992237416\n",
      "train loss:0.027143262548854255\n",
      "train loss:0.010119416343510624\n",
      "train loss:0.010612925042745414\n",
      "train loss:0.021639056422986407\n",
      "train loss:0.03768169087575853\n",
      "train loss:0.005334446542352815\n",
      "train loss:0.008040528634206042\n",
      "train loss:0.03874420672043673\n",
      "train loss:0.04591941516120152\n",
      "train loss:0.013410101314765176\n",
      "train loss:0.009351863749093242\n",
      "train loss:0.025794785735711125\n",
      "train loss:0.007289516985612033\n",
      "train loss:0.022798572367711763\n",
      "train loss:0.015201259502291333\n",
      "train loss:0.018935342545165913\n",
      "train loss:0.03169418348068592\n",
      "train loss:0.010921177956446815\n",
      "train loss:0.025698249150125987\n",
      "train loss:0.032076555454209295\n",
      "train loss:0.006748510537899931\n",
      "train loss:0.008397245493733165\n",
      "train loss:0.02587840930768124\n",
      "train loss:0.012783873073491008\n",
      "train loss:0.014024476972730994\n",
      "train loss:0.014297576901154763\n",
      "train loss:0.019799584099547084\n",
      "train loss:0.023928349977665797\n",
      "train loss:0.006218150758425304\n",
      "train loss:0.06539113782981242\n",
      "train loss:0.06605958791408816\n",
      "train loss:0.0664000372344509\n",
      "train loss:0.02946501941200195\n",
      "train loss:0.047399472785586046\n",
      "train loss:0.036468998451467505\n",
      "train loss:0.007860886696424284\n",
      "train loss:0.047578561483082454\n",
      "train loss:0.0034815427656414446\n",
      "train loss:0.005950569268837872\n",
      "train loss:0.00814116807764876\n",
      "train loss:0.003696601288808602\n",
      "train loss:0.004721080800434765\n",
      "train loss:0.007392321014536066\n",
      "train loss:0.0034614563969693184\n",
      "train loss:0.06701768933206365\n",
      "train loss:0.0848488754622601\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     19\u001b[0m network \u001b[38;5;241m=\u001b[39m SimpleConvNet(input_dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m), \n\u001b[1;32m     20\u001b[0m                         conv_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[1;32m     21\u001b[0m                         hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, weight_init_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[1;32m     24\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mmax_epochs, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     25\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[1;32m     26\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# パラメータの保存\u001b[39;00m\n\u001b[1;32m     30\u001b[0m network\u001b[38;5;241m.\u001b[39msave_params(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/app/common/trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m---> 71\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m/app/common/trainer.py:44\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train[batch_mask]\n\u001b[1;32m     42\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[batch_mask]\n\u001b[0;32m---> 44\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "File \u001b[0;32m/app/simple_convnet.py:126\u001b[0m, in \u001b[0;36mSimpleConvNet.gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"勾配を求める（誤差逆伝搬法）\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    grads['b1']、grads['b2']、...は各層のバイアス\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[1;32m    129\u001b[0m dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/app/simple_convnet.py:71\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"損失関数を求める\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    引数のxは入力データ、tは教師ラベル\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "File \u001b[0;32m/app/simple_convnet.py:63\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 63\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/app/common/layers.py:261\u001b[0m, in \u001b[0;36mPooling.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    258\u001b[0m out_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (H \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_h) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[1;32m    259\u001b[0m out_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (W \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_w) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[0;32m--> 261\u001b[0m col \u001b[38;5;241m=\u001b[39m \u001b[43mim2col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m col \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_h\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_w)\n\u001b[1;32m    264\u001b[0m arg_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/app/common/util.py:65\u001b[0m, in \u001b[0;36mim2col\u001b[0;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(filter_w):\n\u001b[1;32m     64\u001b[0m         x_max \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m stride\u001b[38;5;241m*\u001b[39mout_w\n\u001b[0;32m---> 65\u001b[0m         col[:, :, y, x, :, :] \u001b[38;5;241m=\u001b[39m img[:, :, y:y_max:stride, x:x_max:stride]\n\u001b[1;32m     67\u001b[0m col \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(N\u001b[38;5;241m*\u001b[39mout_h\u001b[38;5;241m*\u001b[39mout_w, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m col\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f29a2-e773-4d9a-9b5e-0fe360c76fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
